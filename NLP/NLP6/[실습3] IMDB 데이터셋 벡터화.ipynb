{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5ZZS6o0Z8Oj"
      },
      "source": [
        "# [실습3] IMDB 영화 후기 데이터셋 벡터화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfwyzgmXZ8Ou"
      },
      "source": [
        "IMDB 데이터셋을 직접 다운로드하여 벡터화하는 과정을 살펴본다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaO7ilwSZ8Ou"
      },
      "source": [
        "준비 과정 1: 데이터셋 다운로드 압축 풀기\n",
        "\n",
        "압축을 풀면 아래 구조의 디렉토리가 생성된다.\n",
        "\n",
        "```\n",
        "aclImdb/\n",
        "...train/\n",
        "......pos/\n",
        "......neg/\n",
        "...test/\n",
        "......pos/\n",
        "......neg/\n",
        "```\n",
        "\n",
        "`train`의 `pos`와 `neg` 서브디렉토리에 각각 12,500개의 긍정과 부정 후기가\n",
        "포함되어 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eScCXFlWZ8Ou",
        "outputId": "e64fb37f-d5a6-49ac-f8b9-0567bb3ce0b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  4301k      0  0:00:19  0:00:19 --:--:-- 4221k\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMRKwRM9Z8Ou"
      },
      "source": [
        "`aclImdb/train/unsup` 서브디렉토리는 필요 없기에 삭제한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2xsKw4vPZ8Ou"
      },
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "if platform.system() == 'Linux':\n",
        "    !rm -r aclImdb/train/unsup\n",
        "else:\n",
        "    import shutil\n",
        "    unsup_path = './aclImdb/train/unsup'\n",
        "    shutil.rmtree(unsup_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rZ3a9ETZ8Ou"
      },
      "source": [
        "긍정 후기 하나의 내용을 살펴보자.\n",
        "모델 구성 이전에 훈련 데이터셋을 살펴 보고\n",
        "모델에 대한 직관을 갖는 과정이 항상 필요하다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK5FRFX0Z8Ou",
        "outputId": "1304a433-6e6f-4ad0-f0e7-80b49147acb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    !cat aclImdb/train/pos/4077_10.txt\n",
        "else:\n",
        "    with open('aclImdb/train/pos/4077_10.txt', 'r') as f:\n",
        "        text = f.read()\n",
        "        print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfeoV0YCZ8Ov"
      },
      "source": [
        "준비 과정 2: 검증셋 준비\n",
        "\n",
        "훈련셋의 20%를 검증셋으로 떼어낸다.\n",
        "이를 위해 `aclImdb/val` 디렉토리를 생성한 후에\n",
        "긍정과 부정 훈련셋 모두 무작위로 섞은 후 그중 20%를 검증셋 디렉토리로 옮긴다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xX30tWVBZ8Ov"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)            # val 디렉토리 생성\n",
        "    files = os.listdir(train_dir / category)\n",
        "\n",
        "    random.Random(1337).shuffle(files)         # 훈련셋 무작위 섞기\n",
        "\n",
        "    num_val_samples = int(0.2 * len(files))    # 20% 지정 후 검증셋으로 옮기기\n",
        "    val_files = files[-num_val_samples:]\n",
        "\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fljBC3puZ8Ov"
      },
      "source": [
        "준비 과정 3: 텐서 데이터셋 준비\n",
        "\n",
        "`text_dataset_from_directory()` 함수를 이용하여\n",
        "훈련셋, 검증셋, 테스트셋을 준비한다.\n",
        "자료형은 모두 `Dataset`이며, 배치 크기는 32를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e41g6BtPZ8Ov",
        "outputId": "8fa59267-238d-44c7-87f4-33df0e3e15b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20000 files belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-02 10:31:28.682533: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
            "2025-06-02 10:31:28.682564: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
            "2025-06-02 10:31:28.682574: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
            "2025-06-02 10:31:28.682602: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2025-06-02 10:31:28.682619: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        "    )\n",
        "\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        "    )\n",
        "\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLi_rBmlZ8Ov"
      },
      "source": [
        "각 데이터셋은 배치로 구분되며\n",
        "입력은 `tf.string` 텐서이고, 타깃은 `int32` 텐서이다.\n",
        "크기는 모두 32이며 지정된 배치 크기이다.\n",
        "예를 들어, 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G05FSfO1Z8Ov",
        "outputId": "8ca36337-3ce3-4a51-d35b-ce16eafbc25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'While Urban Cowboy did not ooze with the same testosterone you might find at a rodeo, it did provide an accurate glimpse of that day and age, in urban Texas. I also think that to truly critique this movie, one would have to have lived in the time and relative place that it was made. There was good music, fun times and, yes, a few \"rough and tumbles\" at the honky tonk roadhouses. The relationship of Bud and Sissy, like \"two ships passing in the night\", was well conceived. When Pam tore up the note that Sissy had written to Bud, it echoed the tragedy of many true life romances. The entire story was well thought out. I thought the cast and crew did an excellent job. I thought the screen play was well written and directed. Scott Glenn should have received an Oscar for best supporting actor.', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "\n",
        "    # 예제: 첫째 배치의 첫째 후기\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM3uJTQVlJSy"
      },
      "source": [
        "### 11.3.3 시퀀스 활용법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXZ34Qg7lJSy"
      },
      "source": [
        "**정수 벡터 데이터셋 준비**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoVifu3DlJSy"
      },
      "source": [
        "훈련셋의 모든 후기 문장을 정수들의 벡터로 변환한다.\n",
        "단, 후기 문장이 최대 600개의 단어만 포함하도록 한다.\n",
        "또한 사용되는 어휘는 빈도 기준 최대 2만 개로 제한한다.\n",
        "\n",
        "- `max_length = 600`\n",
        "- `max_tokens = 20000`\n",
        "- `output_sequence_length=max_length`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "orh58qTYlJSy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-02 10:32:03.773242: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "# 어휘 색인 생성 대상 훈련셋 후기 텍스트 데이터셋\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T94MVnSHlJSy"
      },
      "source": [
        "변환된 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다.\n",
        "`output_sequence_length=600`으로 지정하였기에 모든 문장은 단어를 최대 600개에서\n",
        "잘린다. 따라서 생성되는 정수들의 벡터는 길이가 모두 600으로 지정된다.\n",
        "물론 문장이 600개보다 적은 수의 단어를 사용한다면 나머지는 0으로 채워진다.\n",
        "또한 벡터에 사용된 정수는 2만보다 작은 값이며,\n",
        "이는 빈도가 가장 높은 2만개의 단어만을 대상(`max_tokens=20000`)으로 했기 때문이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B-SbA7rlJSy",
        "outputId": "a951c537-5db8-4bfe-eda4-346b2e4145b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: (32, 600)\n",
            "inputs.dtype: <dtype: 'int64'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(\n",
            "[ 5290  1041    21   123   200   962  1125     5   346    51   148    11\n",
            "   959   420    69   362   470    46     6   391    34  3507     3  4037\n",
            "  1853   762    41    29     5     2    88  1201     3     1   832     1\n",
            "  7982   122    92    70  1239   143  5519     6     1    16     2  1618\n",
            "    80    10   151  1355     6    82    98  2197    31    32     3   317\n",
            "  6847     4   929     3 13241   398   860  1227    17     4 16994     8\n",
            "     4  1970 11735  2249 13702    81   361     2    19   157  6455    43\n",
            "     4 10652   739  1280     1   813  2217    17   959   475     8     4\n",
            "  2207  3000  3263     3  7741     9     6    28    25  4685     6  8081\n",
            "    21   959  3901    37  1000  2285     3    37  1000  4686 11640  1292\n",
            "    32   629    51    27  5168  1970   466    25  4848     8     4  2249\n",
            "   513   466     1    25     1  1302  2898    31     4     1   180   582\n",
            "   101    12    27    42  1415    76     4  1032 10493    73     6     2\n",
            " 12938     5    25  1154   644   813  3014     6  9131    29  3244    10\n",
            "  1336   371   134   768   439  1090    11  1015  4075   197    19    18\n",
            "   117     9    63    26     6    28    38   363    48    24   427  5268\n",
            "     3  1503  2167   514     5     2    20     8    60   411   159   554\n",
            "    10    69  3428     2     1     5   653     3  3419  8839    18    11\n",
            "    19   146    54     1    36    34  4456   864    41  5243  5071   113\n",
            "  1274    17     2  2766  1353     5     2  2073    10   217   919    79\n",
            "    10    59    26   597     2  1135    52   649    45     2  6266  2276\n",
            "    59  1141   300    21     1 17918     2  5541   483    27   146     3\n",
            "  6823  1060     1  1472    87    24    23   405     6   189  3562  3192\n",
            "     3   190   959   762   639    51  1970   146    54  3557    29  3894\n",
            "   516   565    12    59    26    92   959   420     4   339  3252   150\n",
            "    30    42     4  2594     3 14626   399     5  2558     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0], shape=(600,), dtype=int64)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in int_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCrjK1-zUwFj"
      },
      "source": [
        "**트랜스포머 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH2T-AQOUwFj"
      },
      "source": [
        "위 그림에서 설명된 트랜스포머 인코더를 층(layer)으로 구현하면 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X6g33CzrUwFk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtXomfGDUwFk"
      },
      "source": [
        "**트랜스포머 인코더 활용 모델**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPSowlcGUwFk"
      },
      "source": [
        "훈련 데이터셋이 입력되면 먼저 단어 임베딩을 이용하여\n",
        "단어들 사이의 연관성을 찾는다.\n",
        "이후 트랜스포머 인코더로 셀프 어텐션을 적용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxbIVhi3UwFk",
        "outputId": "77b51533-c428-4b80-b704-24227fc076cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " transformer_encoder_2 (Tra  (None, None, 256)         543776    \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " global_max_pooling1d_2 (Gl  (None, 256)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5664033 (21.61 MB)\n",
            "Trainable params: 5664033 (21.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "# 길이가 600인 1차원 어레이로 변환\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rJ82rMIUwFl"
      },
      "source": [
        "훈련 과정은 특별한 게 없다.\n",
        "테스트셋에 대한 정확도가 87.5% 정도로 바이그램 모델보다 좀 더 낮다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2BDEYHaUwFl",
        "outputId": "a81a5913-fdf8-44f9-f2ce-d4f983479c51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-02 10:32:21.739187: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - ETA: 0s - loss: 0.4676 - accuracy: 0.7875"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-02 10:36:26.860206: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: transformer_encoder/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: transformer_encoder/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 271s 430ms/step - loss: 0.4676 - accuracy: 0.7875 - val_loss: 0.3040 - val_accuracy: 0.8712\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.3132 - accuracy: 0.8679INFO:tensorflow:Assets written to: transformer_encoder/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: transformer_encoder/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 266s 425ms/step - loss: 0.3132 - accuracy: 0.8679 - val_loss: 0.2684 - val_accuracy: 0.8892\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 265s 424ms/step - loss: 0.2685 - accuracy: 0.8900 - val_loss: 0.2713 - val_accuracy: 0.8872\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 262s 420ms/step - loss: 0.2258 - accuracy: 0.9118 - val_loss: 0.3955 - val_accuracy: 0.8442\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.1904 - accuracy: 0.9255INFO:tensorflow:Assets written to: transformer_encoder/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: transformer_encoder/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 263s 422ms/step - loss: 0.1904 - accuracy: 0.9255 - val_loss: 0.2508 - val_accuracy: 0.9014\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 265s 424ms/step - loss: 0.1546 - accuracy: 0.9413 - val_loss: 0.3495 - val_accuracy: 0.8760\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 265s 424ms/step - loss: 0.1234 - accuracy: 0.9539 - val_loss: 0.3344 - val_accuracy: 0.8814\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 259s 415ms/step - loss: 0.1032 - accuracy: 0.9603 - val_loss: 0.3354 - val_accuracy: 0.8836\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 260s 416ms/step - loss: 0.0817 - accuracy: 0.9700 - val_loss: 0.3590 - val_accuracy: 0.8868\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 268s 429ms/step - loss: 0.0672 - accuracy: 0.9754 - val_loss: 0.3739 - val_accuracy: 0.8920\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 269s 430ms/step - loss: 0.0568 - accuracy: 0.9800 - val_loss: 0.3874 - val_accuracy: 0.8938\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 270s 433ms/step - loss: 0.0437 - accuracy: 0.9834 - val_loss: 0.4514 - val_accuracy: 0.8796\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 266s 425ms/step - loss: 0.0337 - accuracy: 0.9872 - val_loss: 0.6974 - val_accuracy: 0.8490\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 265s 423ms/step - loss: 0.0294 - accuracy: 0.9898 - val_loss: 0.5393 - val_accuracy: 0.8818\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 266s 425ms/step - loss: 0.0206 - accuracy: 0.9928 - val_loss: 0.5756 - val_accuracy: 0.8758\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 265s 423ms/step - loss: 0.0169 - accuracy: 0.9941 - val_loss: 0.6581 - val_accuracy: 0.8776\n",
            "Epoch 17/20\n",
            "625/625 [==============================] - 268s 429ms/step - loss: 0.0139 - accuracy: 0.9949 - val_loss: 0.6858 - val_accuracy: 0.8766\n",
            "Epoch 18/20\n",
            "625/625 [==============================] - 269s 431ms/step - loss: 0.0087 - accuracy: 0.9966 - val_loss: 0.9697 - val_accuracy: 0.8552\n",
            "Epoch 19/20\n",
            "625/625 [==============================] - 262s 419ms/step - loss: 0.0092 - accuracy: 0.9973 - val_loss: 0.9146 - val_accuracy: 0.8574\n",
            "Epoch 20/20\n",
            "625/625 [==============================] - 259s 415ms/step - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.8272 - val_accuracy: 0.8702\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-02 12:00:46.849385: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 126s 161ms/step - loss: 0.3170 - accuracy: 0.8636\n",
            "Test acc: 0.864\n"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_encoder\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_encoder\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tEOxNV_UwFl"
      },
      "source": [
        "**단어 위치 인코딩**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHQszXy-UwFl"
      },
      "source": [
        "다음 `PositionalEmbedding` 층 클래스는 두 개의 임베딩 클래스를 사용한다.\n",
        "하나는 보통의 단어 임베딩이며,\n",
        "다른 하나는 단어의 위치 정보를 임베딩한다.\n",
        "각 임베딩의 출력값을 합친 값을 트랜스포머에게 전달하는 역할을 수행한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uRiJ-vBXUwFm"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG2Db4SRUwFm"
      },
      "source": [
        "**단어위치인식 트랜스포머 아키텍처**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq24k0wlUwFm"
      },
      "source": [
        "아래 코드는 `PositionalEmbedding` 층을 활용하여 트랜스포머 인코더가\n",
        "단어위치를 활용할 수 있도록 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jYcxHlk7UwFm"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTqdNRiliiYQ",
        "outputId": "ba70a092-839a-4307-ec8a-a28610059829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " positional_embedding (Posi  (None, None, 256)         5273600   \n",
            " tionalEmbedding)                                                \n",
            "                                                                 \n",
            " transformer_encoder_3 (Tra  (None, None, 256)         543776    \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " global_max_pooling1d_3 (Gl  (None, 256)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5817633 (22.19 MB)\n",
            "Trainable params: 5817633 (22.19 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt5VbzzNiiYQ",
        "outputId": "ba70a092-839a-4307-ec8a-a28610059829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-02 12:10:27.631887: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - ETA: 0s - loss: 0.4762 - accuracy: 0.7775"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-02 12:14:44.913449: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: full_transformer_encoder/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: full_transformer_encoder/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 285s 452ms/step - loss: 0.4762 - accuracy: 0.7775 - val_loss: 0.2552 - val_accuracy: 0.8896\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 283s 453ms/step - loss: 0.2469 - accuracy: 0.9023 - val_loss: 0.2701 - val_accuracy: 0.9020\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 276s 441ms/step - loss: 0.1798 - accuracy: 0.9306 - val_loss: 0.3081 - val_accuracy: 0.9004\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 276s 442ms/step - loss: 0.1461 - accuracy: 0.9429 - val_loss: 0.4002 - val_accuracy: 0.8898\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 278s 445ms/step - loss: 0.1213 - accuracy: 0.9553 - val_loss: 0.3918 - val_accuracy: 0.8932\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 278s 445ms/step - loss: 0.1067 - accuracy: 0.9610 - val_loss: 0.4404 - val_accuracy: 0.8940\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 275s 441ms/step - loss: 0.0896 - accuracy: 0.9687 - val_loss: 0.4930 - val_accuracy: 0.8928\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 270s 432ms/step - loss: 0.0731 - accuracy: 0.9733 - val_loss: 0.6371 - val_accuracy: 0.8804\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 274s 439ms/step - loss: 0.0586 - accuracy: 0.9797 - val_loss: 0.5830 - val_accuracy: 0.8920\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 271s 434ms/step - loss: 0.0480 - accuracy: 0.9830 - val_loss: 0.6200 - val_accuracy: 0.8850\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 269s 430ms/step - loss: 0.0429 - accuracy: 0.9863 - val_loss: 0.8718 - val_accuracy: 0.8524\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 264s 422ms/step - loss: 0.0357 - accuracy: 0.9872 - val_loss: 0.7266 - val_accuracy: 0.8780\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 266s 426ms/step - loss: 0.0312 - accuracy: 0.9908 - val_loss: 0.7780 - val_accuracy: 0.8688\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 263s 420ms/step - loss: 0.0257 - accuracy: 0.9922 - val_loss: 0.7868 - val_accuracy: 0.8786\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 263s 422ms/step - loss: 0.0283 - accuracy: 0.9916 - val_loss: 0.7059 - val_accuracy: 0.8754\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 266s 425ms/step - loss: 0.0242 - accuracy: 0.9929 - val_loss: 0.7970 - val_accuracy: 0.8780\n",
            "Epoch 17/20\n",
            "625/625 [==============================] - 271s 433ms/step - loss: 0.0197 - accuracy: 0.9940 - val_loss: 0.8072 - val_accuracy: 0.8778\n",
            "Epoch 18/20\n",
            "625/625 [==============================] - 264s 422ms/step - loss: 0.0190 - accuracy: 0.9943 - val_loss: 0.7754 - val_accuracy: 0.8786\n",
            "Epoch 19/20\n",
            "625/625 [==============================] - 264s 423ms/step - loss: 0.0154 - accuracy: 0.9949 - val_loss: 1.1046 - val_accuracy: 0.8578\n",
            "Epoch 20/20\n",
            "625/625 [==============================] - 263s 422ms/step - loss: 0.0109 - accuracy: 0.9967 - val_loss: 1.1007 - val_accuracy: 0.8668\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-02 13:40:50.857773: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 129s 165ms/step - loss: 0.5904 - accuracy: 0.6736\n",
            "Test acc: 0.674\n"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tf-mac",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
