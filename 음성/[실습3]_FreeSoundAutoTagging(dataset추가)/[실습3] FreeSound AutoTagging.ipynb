{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "meUTU9We3o56"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 7767,
     "status": "ok",
     "timestamp": 1590857362501,
     "user": {
      "displayName": "도승헌",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64",
      "userId": "15552087445877711483"
     },
     "user_tz": -540
    },
    "id": "5KkwDCJB10Tq",
    "outputId": "aacefca2-837e-4446-b0bc-8a9277c076e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mels_train_curated.pkl', 'sample_submission.csv', 'train_curated.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = \"./dataset\"\n",
    "filename = os.listdir(datapath)\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "executionInfo": {
     "elapsed": 7845,
     "status": "ok",
     "timestamp": 1590857362961,
     "user": {
      "displayName": "도승헌",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64",
      "userId": "15552087445877711483"
     },
     "user_tz": -540
    },
    "id": "a-P0IrDoUcQ0",
    "outputId": "fa1d90b4-c30c-4901-d361-c9f1bdd1998b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0006ae4e.wav</td>\n",
       "      <td>Bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0019ef41.wav</td>\n",
       "      <td>Raindrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ec0ad.wav</td>\n",
       "      <td>Finger_snapping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0026c7cb.wav</td>\n",
       "      <td>Run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0026f116.wav</td>\n",
       "      <td>Finger_snapping</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fname           labels\n",
       "0  0006ae4e.wav             Bark\n",
       "1  0019ef41.wav         Raindrop\n",
       "2  001ec0ad.wav  Finger_snapping\n",
       "3  0026c7cb.wav              Run\n",
       "4  0026f116.wav  Finger_snapping"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(os.path.join(datapath, filename[2]))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 7580,
     "status": "ok",
     "timestamp": 1590857362963,
     "user": {
      "displayName": "도승헌",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64",
      "userId": "15552087445877711483"
     },
     "user_tz": -540
    },
    "id": "Kv0LxV7iUj8z",
    "outputId": "43d0abaf-24dd-4088-c9e3-4b365bced516"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = test_df.columns[1:].tolist()\n",
    "num_classes = len(labels)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7306,
     "status": "ok",
     "timestamp": 1590857362964,
     "user": {
      "displayName": "도승헌",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64",
      "userId": "15552087445877711483"
     },
     "user_tz": -540
    },
    "id": "Yu2x_1SA95_l",
    "outputId": "b46594d8-499d-443b-81f3-fa1e2613415c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 7997,
     "status": "ok",
     "timestamp": 1590857363939,
     "user": {
      "displayName": "도승헌",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64",
      "userId": "15552087445877711483"
     },
     "user_tz": -540
    },
    "id": "fpAAUBnZ2Jp_",
    "outputId": "eccf904f-8b1b-4548-f09a-f6f523d8ab81"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>Accelerating_and_revving_and_vroom</th>\n",
       "      <th>Accordion</th>\n",
       "      <th>Acoustic_guitar</th>\n",
       "      <th>Applause</th>\n",
       "      <th>Bark</th>\n",
       "      <th>Bass_drum</th>\n",
       "      <th>Bass_guitar</th>\n",
       "      <th>Bathtub_(filling_or_washing)</th>\n",
       "      <th>Bicycle_bell</th>\n",
       "      <th>...</th>\n",
       "      <th>Toilet_flush</th>\n",
       "      <th>Traffic_noise_and_roadway_noise</th>\n",
       "      <th>Trickle_and_dribble</th>\n",
       "      <th>Walk_and_footsteps</th>\n",
       "      <th>Water_tap_and_faucet</th>\n",
       "      <th>Waves_and_surf</th>\n",
       "      <th>Whispering</th>\n",
       "      <th>Writing</th>\n",
       "      <th>Yell</th>\n",
       "      <th>Zipper_(clothing)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4260ebea.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>426eb1e0.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>428d70bb.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4292b1c9.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>429c5071.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          fname  Accelerating_and_revving_and_vroom  Accordion  \\\n",
       "0  4260ebea.wav                                   0          0   \n",
       "1  426eb1e0.wav                                   0          0   \n",
       "2  428d70bb.wav                                   0          0   \n",
       "3  4292b1c9.wav                                   0          0   \n",
       "4  429c5071.wav                                   0          0   \n",
       "\n",
       "   Acoustic_guitar  Applause  Bark  Bass_drum  Bass_guitar  \\\n",
       "0                0         0     0          0            0   \n",
       "1                0         0     0          0            0   \n",
       "2                0         0     0          0            0   \n",
       "3                0         0     0          0            0   \n",
       "4                0         0     0          0            0   \n",
       "\n",
       "   Bathtub_(filling_or_washing)  Bicycle_bell  ...  Toilet_flush  \\\n",
       "0                             0             0  ...             0   \n",
       "1                             0             0  ...             0   \n",
       "2                             0             0  ...             0   \n",
       "3                             0             0  ...             0   \n",
       "4                             0             0  ...             0   \n",
       "\n",
       "   Traffic_noise_and_roadway_noise  Trickle_and_dribble  Walk_and_footsteps  \\\n",
       "0                                0                    0                   0   \n",
       "1                                0                    0                   0   \n",
       "2                                0                    0                   0   \n",
       "3                                0                    0                   0   \n",
       "4                                0                    0                   0   \n",
       "\n",
       "   Water_tap_and_faucet  Waves_and_surf  Whispering  Writing  Yell  \\\n",
       "0                     0               0           0        0     0   \n",
       "1                     0               0           0        0     0   \n",
       "2                     0               0           0        0     0   \n",
       "3                     0               0           0        0     0   \n",
       "4                     0               0           0        0     0   \n",
       "\n",
       "   Zipper_(clothing)  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_curated = pd.read_csv(os.path.join(datapath, filename[1]))\n",
    "train_curated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NfqDfBMO2jxc"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(datapath, filename[0]), 'rb') as curated:\n",
    "    x_train = pickle.load(curated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 19841,
     "status": "ok",
     "timestamp": 1590857376449,
     "user": {
      "displayName": "도승헌",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64",
      "userId": "15552087445877711483"
     },
     "user_tz": -540
    },
    "id": "TKc-ipgB-EMC",
    "outputId": "1043f61b-c7fd-4b1b-e078-ddd31c49eb7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 785, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 19617,
     "status": "ok",
     "timestamp": 1590857376450,
     "user": {
      "displayName": "도승헌",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64",
      "userId": "15552087445877711483"
     },
     "user_tz": -540
    },
    "id": "FncliWX924lE",
    "outputId": "9dc75138-3bbd-4f9b-8c3b-1b389c083d42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4970, 3361)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(train_curated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fname', 'Accelerating_and_revving_and_vroom', 'Accordion', 'Acoustic_guitar', 'Applause', 'Bark', 'Bass_drum', 'Bass_guitar', 'Bathtub_(filling_or_washing)', 'Bicycle_bell', 'Burping_and_eructation', 'Bus', 'Buzz', 'Car_passing_by', 'Cheering', 'Chewing_and_mastication', 'Child_speech_and_kid_speaking', 'Chink_and_clink', 'Chirp_and_tweet', 'Church_bell', 'Clapping', 'Computer_keyboard', 'Crackle', 'Cricket', 'Crowd', 'Cupboard_open_or_close', 'Cutlery_and_silverware', 'Dishes_and_pots_and_pans', 'Drawer_open_or_close', 'Drip', 'Electric_guitar', 'Fart', 'Female_singing', 'Female_speech_and_woman_speaking', 'Fill_(with_liquid)', 'Finger_snapping', 'Frying_(food)', 'Gasp', 'Glockenspiel', 'Gong', 'Gurgling', 'Harmonica', 'Hi-hat', 'Hiss', 'Keys_jangling', 'Knock', 'Male_singing', 'Male_speech_and_man_speaking', 'Marimba_and_xylophone', 'Mechanical_fan', 'Meow', 'Microwave_oven', 'Motorcycle', 'Printer', 'Purr', 'Race_car_and_auto_racing', 'Raindrop', 'Run', 'Scissors', 'Screaming', 'Shatter', 'Sigh', 'Sink_(filling_or_washing)', 'Skateboard', 'Slam', 'Sneeze', 'Squeak', 'Stream', 'Strum', 'Tap', 'Tick-tock', 'Toilet_flush', 'Traffic_noise_and_roadway_noise', 'Trickle_and_dribble', 'Walk_and_footsteps', 'Water_tap_and_faucet', 'Waves_and_surf', 'Whispering', 'Writing', 'Yell', 'Zipper_(clothing)']\n"
     ]
    }
   ],
   "source": [
    "print(train_curated.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 19355,
     "status": "ok",
     "timestamp": 1590857376450,
     "user": {
      "displayName": "도승헌",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64",
      "userId": "15552087445877711483"
     },
     "user_tz": -540
    },
    "id": "Gpx-Zld_U67S",
    "outputId": "ecdf6f4b-cd31-4849-ac8f-77fff63b3021"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3361, 80)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train = np.zeros((len(train_curated), num_classes)).astype(int)\n",
    "#for i, row in enumerate(train_curated['labels'].str.split(',')):\n",
    "#    for label in row:\n",
    "#        idx = labels.index(label)\n",
    "#        y_train[i, idx] = 1\n",
    "labels = train_curated.columns[1:]\n",
    "num_classes = len(labels)\n",
    "y_train = train_curated[labels].values.astype(int)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oUoIxjN9v0Of"
   },
   "outputs": [],
   "source": [
    "def _one_sample_positive_class_precisions(scores, truth):\n",
    "    \"\"\"단일 샘플에 대한 각 실제 클래스의 정밀도를 계산합니다.\n",
    "    Args:\n",
    "      scores: np.array of (num_classes,) giving the individual classifier scores.\n",
    "      truth: np.array of (num_classes,) bools indicating which classes are true.\n",
    "    Returns:\n",
    "      pos_class_indices: np.array of indices of the true classes for this sample.\n",
    "      pos_class_precisions: np.array of precisions corresponding to each of those\n",
    "        classes.\n",
    "    \"\"\"\n",
    "    num_classes = scores.shape[0]\n",
    "    pos_class_indices = np.flatnonzero(truth > 0)\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "    class_rankings = np.zeros(num_classes, dtype=int)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=bool)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(float)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "\n",
    "def calculate_per_class_lwlrap(truth, scores):\n",
    "    \"\"\"Calculate label-weighted label-ranking average precision.\n",
    "\n",
    "    Arguments:\n",
    "      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n",
    "        of presence of that class in that sample.\n",
    "      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n",
    "        test's real-valued score for each class for each sample.\n",
    "\n",
    "    Returns:\n",
    "      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n",
    "        class.\n",
    "      weight_per_class: np.array of (num_classes,) giving the prior of each\n",
    "        class within the truth labels.  Then the overall unbalanced lwlrap is\n",
    "        simply np.sum(per_class_lwlrap * weight_per_class)\n",
    "    \"\"\"\n",
    "    assert truth.shape == scores.shape\n",
    "    num_samples, num_classes = scores.shape\n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = (\n",
    "            _one_sample_positive_class_precisions(scores[sample_num, :],\n",
    "                                                  truth[sample_num, :]))\n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n",
    "            precision_at_hits)\n",
    "    labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                        np.maximum(1, labels_per_class))\n",
    "\n",
    "    return per_class_lwlrap, weight_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 18875,
     "status": "ok",
     "timestamp": 1590857376452,
     "user": {
      "displayName": "도승헌",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64",
      "userId": "15552087445877711483"
     },
     "user_tz": -540
    },
    "id": "JG9T-ttYviKl",
    "outputId": "2568c460-b4b8-4305-b935-d8645a146f43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 2]), array([1.        , 0.66666667]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_one_sample_positive_class_precisions(np.array([0.7,0.3,0.1]),np.array([1,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "executionInfo": {
     "elapsed": 18428,
     "status": "ok",
     "timestamp": 1590857376452,
     "user": {
      "displayName": "도승헌",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64",
      "userId": "15552087445877711483"
     },
     "user_tz": -540
    },
    "id": "1Kgd6rRgvzDY",
    "outputId": "93d36009-4bdb-457f-82fb-fe7354487306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 1 Score [0.66666667 0.5       ]\n",
      "sample 2 Score [1. 1.]\n",
      "Each class score [0.66666667 1.         0.75      ]\n",
      "Weight of each class [0.25 0.25 0.5 ]\n",
      "LwLRAP 0.7916666666666666\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([[1, 0, 1,], [0, 1, 1]])\n",
    "y_score = np.array([[0.1, 0.7, 0.2], [0.1, 0.7, 0.2]])\n",
    "_, precision_at_hits1 = _one_sample_positive_class_precisions(y_score[0], y_true[0])\n",
    "print(\"sample 1 Score\", precision_at_hits1)\n",
    "_, precision_at_hits2 = _one_sample_positive_class_precisions(y_score[1], y_true[1])\n",
    "print(\"sample 2 Score\", precision_at_hits2)\n",
    "score, weight = calculate_per_class_lwlrap(y_true, y_score)\n",
    "print(\"Each class score\", score)\n",
    "print(\"Weight of each class\", weight)\n",
    "LwLRAP = (score*weight).sum()\n",
    "print(\"LwLRAP\", LwLRAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvGt9fbQbpnD"
   },
   "source": [
    "https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ccbw_ZWDwsHQ"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 8, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=4)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=4)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(64, out_channels, kernel_size=3, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1NmJpLQC0zOE"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            ConvBlock(in_channels=3, out_channels=64),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "P9ly47qu01I7"
   },
   "outputs": [],
   "source": [
    "class FATTrainDataset(Dataset):\n",
    "    def __init__(self, mels, labels):\n",
    "        super().__init__()\n",
    "        self.mels = mels\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio = self.mels[idx]\n",
    "        audio_clip = audio[:,:128,:]\n",
    "        label = self.labels[idx]\n",
    "        label = torch.from_numpy(label).float()\n",
    "        audio_clip = torch.Tensor(audio_clip)\n",
    "\n",
    "        return audio_clip, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:len(y_train)]  # 3361개로 잘라서 맞춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "FpCdYuWa4cng"
   },
   "outputs": [],
   "source": [
    "num_classes = y_train.shape[1]\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "_HtICLnn42VO"
   },
   "outputs": [],
   "source": [
    "train_dataset = FATTrainDataset(x_trn, y_trn)\n",
    "valid_dataset = FATTrainDataset(x_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 1051,
     "status": "ok",
     "timestamp": 1590857401107,
     "user": {
      "displayName": "도승헌",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64",
      "userId": "15552087445877711483"
     },
     "user_tz": -540
    },
    "id": "6avs4ercfPbs",
    "outputId": "e9de77ee-e5b4-4894-b951-e0e5e40d3e36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "fVNNCmpoVK6R"
   },
   "outputs": [],
   "source": [
    "best_epoch = -1\n",
    "best_lwlrap = 0.\n",
    "num_epochs = 80\n",
    "batch_size = 8\n",
    "test_batch_size = 64\n",
    "lr = 3e-3\n",
    "\n",
    "model = Classifier(num_classes=num_classes).cpu()\n",
    "criterion = nn.BCEWithLogitsLoss().cpu()\n",
    "optimizer = Adam(params=model.parameters(), lr=lr, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "executionInfo": {
     "elapsed": 450823,
     "status": "ok",
     "timestamp": 1590734629937,
     "user": {
      "displayName": "도승헌",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64",
      "userId": "15552087445877711483"
     },
     "user_tz": -540
    },
    "id": "7yeKuQ1DVQqv",
    "outputId": "8b21d72c-7df5-40d4-e2bd-86e1418e8fde"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Epoch 5 - avg_train_loss: 0.0017  avg_val_loss: 0.0013  val_lwlrap: nan  time: 4s<p>Epoch 10 - avg_train_loss: 0.0005  avg_val_loss: 0.0004  val_lwlrap: nan  time: 4s<p>Epoch 15 - avg_train_loss: 0.0003  avg_val_loss: 0.0002  val_lwlrap: nan  time: 4s<p>Epoch 20 - avg_train_loss: 0.0002  avg_val_loss: 0.0001  val_lwlrap: nan  time: 3s<p>Epoch 25 - avg_train_loss: 0.0001  avg_val_loss: 0.0001  val_lwlrap: nan  time: 4s<p>Epoch 30 - avg_train_loss: 0.0001  avg_val_loss: 0.0001  val_lwlrap: nan  time: 4s<p>Epoch 35 - avg_train_loss: 0.0001  avg_val_loss: 0.0000  val_lwlrap: nan  time: 4s<p>Epoch 40 - avg_train_loss: 0.0000  avg_val_loss: 0.0000  val_lwlrap: nan  time: 5s<p>Epoch 45 - avg_train_loss: 0.0000  avg_val_loss: 0.0000  val_lwlrap: nan  time: 4s<p>Epoch 50 - avg_train_loss: 0.0000  avg_val_loss: 0.0000  val_lwlrap: nan  time: 3s<p>Epoch 55 - avg_train_loss: 0.0000  avg_val_loss: 0.0000  val_lwlrap: nan  time: 3s<p>Epoch 60 - avg_train_loss: 0.0000  avg_val_loss: 0.0000  val_lwlrap: nan  time: 3s<p>Epoch 65 - avg_train_loss: 0.0000  avg_val_loss: 0.0000  val_lwlrap: nan  time: 4s<p>Epoch 70 - avg_train_loss: 0.0000  avg_val_loss: 0.0000  val_lwlrap: nan  time: 4s<p>Epoch 75 - avg_train_loss: 0.0000  avg_val_loss: 0.0000  val_lwlrap: nan  time: 4s<p>Epoch 80 - avg_train_loss: 0.0000  avg_val_loss: 0.0000  val_lwlrap: nan  time: 4s"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/p1yx5lj17yx384d74tkfr6480000gn/T/ipykernel_60918/1720094775.py:53: RuntimeWarning: invalid value encountered in divide\n",
      "  weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n"
     ]
    }
   ],
   "source": [
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "mb = master_bar(range(num_epochs))\n",
    "for epoch in mb:\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "\n",
    "    for x_batch, y_batch in progress_bar(train_loader, parent=mb):\n",
    "        preds = model(x_batch.cpu())\n",
    "        loss = criterion(preds, y_batch.cpu())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    valid_preds = np.zeros((len(x_val), num_classes))\n",
    "    avg_val_loss = 0.\n",
    "\n",
    "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "        preds = model(x_batch.cpu()).detach()\n",
    "        loss = criterion(preds, y_batch.cpu())\n",
    "\n",
    "        preds = torch.sigmoid(preds)\n",
    "        valid_preds[i * test_batch_size: (i+1) * test_batch_size] = preds.cpu().numpy()\n",
    "\n",
    "        avg_val_loss += loss.item() / len(valid_loader)\n",
    "\n",
    "    score, weight = calculate_per_class_lwlrap(y_val, valid_preds)\n",
    "    lwlrap = (score * weight).sum()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        mb.write(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  val_lwlrap: {lwlrap:.6f}  time: {elapsed:.0f}s')\n",
    "\n",
    "    if lwlrap > best_lwlrap:\n",
    "        best_epoch = epoch + 1\n",
    "        best_lwlrap = lwlrap\n",
    "        torch.save(model.state_dict(), 'weight_best.pt')\n",
    "\n",
    "temp = {\n",
    "    'best_epoch': best_epoch,\n",
    "    'best_lwlrap': best_lwlrap,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1nJ_s50MHWg2t6L3iNJXmMmMhhtA9wjul",
     "timestamp": 1724673907730
    }
   ]
  },
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
