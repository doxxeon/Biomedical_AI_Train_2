{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습6] 인공지능을 활용한 건설로봇 Motion Control (정답)\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 목표\n",
    "\n",
    "- Gymnasium 라이브러리를 사용하는 방법을 학습합니다.\n",
    "- Inverted pendulum, Cartpole 환경을 사용하는 방법을 학습합니다.\n",
    "- 고전적인 제어기로 비선형 모델들을 제어해봅니다.\n",
    "- 강화학습 알고리즘을 직접 구현해보고 학습시켜봅니다.\n",
    "- 여러가지 강화학습 알고리즘과 성능을 비교해봅니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 목차\n",
    "\n",
    "1. **Gymnasium 라이브러리 사용하기** \n",
    "\n",
    "2. **고전 제어기로 비선형 모델 제어** \n",
    "\n",
    "3. **모델 기반 제어기로 비선형 모델 제어**\n",
    "\n",
    "4. **Reinforce 강화학습 알고리즘 적용** \n",
    "\n",
    "5. **Actor-Critic 강화학습 알고리즘 적용** \n",
    "\n",
    "6. **A2C 강화학습 알고리즘 적용**\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 개요\n",
    "\n",
    "이번 실습에서는 여러가지 강화학습 알고리즘을 직접 구현하고 학습하는 것을 목표로 합니다. 이를 위해 Gymnasium 라이브러리를 사용하는 방법을 학습합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gymnasium 라이브러리 사용하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.1 필수 라이브러리 불러오기\n",
    "강화학습 알고리즘을 학습하고 테스트해보기 위해 여러 가지 시뮬레이션을 사용할 수 있습니다.\n",
    "\n",
    "그 중, 가장 간단하고 추가적인 엔진 설치가 필요 없는 Gymnasium 라이브러리를 소개합니다.\n",
    "\n",
    "Gymnasium은 시뮬레이터와 연동시켜 강화학습 알고리즘을 학습시키기 위한 기본적인 라이브러리입니다. 하지만, 추가적인 시뮬레이터가 없더라도, 기본적으로 Gymnasium 내부에서 제공하고 있는 간단한 시뮬레이션들이 있습니다. \n",
    "\n",
    "이번 실습에서는 Inverted Pendulum과 Cartpole 모델을 사용하겠습니다. 이 두 비선형 모델은 강화학습 뿐만 아니라 대부분의 로봇 제어기의 벤치마크로 활용되는 대표적인 모델들입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 라이브러리 불러오기\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import gymnasium as gym\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.2 Inverted Pendulum 모델 사용하기\n",
    "Inverted Pendulum 모델과 상호작용 하는 방법을 소개합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook 환경에서 Gymnasium의 상호작용 시각화 윈도우를 확인하려면 별도의 코드를 사용해야 합니다. IPython의 display 기능을 사용할 수 있도록 함수를 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_env(env):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(env.render())\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    plt.close()\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상호작용할 수 있는 시뮬레이션을 환경(env)라고 합니다. 라이브러리 내에 내장되어 있는 invertd pendulum 모델을 불러옵니다.\n",
    "\n",
    "Env를 설정하면 로봇의 모델(동역학), 센서(observation), 제어기(action_space)등이 결정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "Action Space: Box(-2.0, 2.0, (1,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Action Space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설정된 환경을 시각화해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation = env.reset(seed=random_seed)\n",
    "render_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환경에 제어 입력을 가해보겠습니다. 랜덤한 제어 입력을 샘플한 후, 이를 환경에 가해봅니다.\n",
    "\n",
    "동역학에 제어 입력이 입력된 후, 다음 로봇 상태가 계산됩니다. \n",
    "\n",
    "연속된 동작을 확인하기 위해 100 번의 제어 입력을 가해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # Random action for demonstration\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    render_env(env)\n",
    "    if terminated or truncated:\n",
    "        observation = env.reset(seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.3 Cartpole 모델 사용하기\n",
    "Cartpole 모델과 상호작용 하는 방법을 소개합니다. 기본적인 방법은 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action Space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Action Space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation = env.reset(seed=random_seed)\n",
    "render_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TODO] 설정한 환경 env를 한 스텝 실행해보세요. 한 스텝을 진행하기 위해서는 제어 입력을 인자로 넣어줘야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # Random action for demonstration\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    render_env(env)\n",
    "    if terminated or truncated:\n",
    "        observation = env.reset(seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cartpole은 가운대의 막대가 떨어지면 실패로 간주합니다. 실패를 하더라도 환경을 리셋하지 않게하면 보다 연속된 동작을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # Random action for demonstration\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    render_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 고전 제어기로 비선형 모델 제어\n",
    "\n",
    "---\n",
    "### 2.1 고전 제어기 설계\n",
    "강화학습 실습에 들어가기 앞서, 고전 제어기의 한계점을 분석해보겠습니다.\n",
    "\n",
    "고전 제어기는 일반적으로 목표하는 desired state와 현재 상태 current state의 차이를 에러로 정의합니다. 그 에러를 최소화하는 것을 제어기의 목표로 설정합니다.\n",
    "\n",
    "고전 제어기는 PID, LQR, SMC 등 많은 방법들이 연구되어 왔습니다.\n",
    "\n",
    "가장 간단한 불연속한 제어기부터 설계해보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Create the Pendulum environments\n",
    "env_pendulum = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "obs = env_pendulum.reset(seed=random_seed)[0]\n",
    "num_steps = 200\n",
    "\n",
    "# Simple controller for Pendulum\n",
    "def simple_controller_pendulum(obs, max_torque=2.0):\n",
    "    # observation에서 theta를 추출\n",
    "    cos_theta = obs[0]\n",
    "    sin_theta = obs[1]\n",
    "    theta = np.arctan2(sin_theta, cos_theta)\n",
    "\n",
    "    if theta > 0.0:\n",
    "        # pendulum이 왼쪽에 있을 때, 시계 방향으로 힘을 가함\n",
    "        action = [-max_torque]\n",
    "    else:\n",
    "        # pendulum이 오른쪽에 있을 때, 반시계 방향으로 힘을 가함\n",
    "        action = [max_torque]\n",
    "    return action\n",
    "\n",
    "episode_reward = 0\n",
    "for step in range(num_steps):\n",
    "    action = simple_controller_pendulum(obs, max_torque=1.0)\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env_pendulum.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "    render_env(env_pendulum)\n",
    "\n",
    "env_pendulum.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 논리로 만든 제어기는 최대 토크가 1일 때는 전혀 작동하지 않는 것을 확인할 수 있습니다.\n",
    "\n",
    "환경에서 허용하는 최대 토크는 2입니다. 2를 가하면 중력의 도움없이도 강제로 막대를 제어할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env_pendulum.reset(seed=random_seed)[0]\n",
    "num_steps = 200\n",
    "\n",
    "episode_reward = 0\n",
    "for step in range(num_steps):\n",
    "    action = simple_controller_pendulum(obs, max_torque=2.0)\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env_pendulum.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "    render_env(env_pendulum)\n",
    "\n",
    "env_pendulum.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "### 2.2 PID 제어기 설계\n",
    "\n",
    "다음은 PID 제어기를 설계해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIDController:\n",
    "    def __init__(self, Kp, Ki, Kd):\n",
    "        self.Kp = Kp\n",
    "        self.Ki = Ki\n",
    "        self.Kd = Kd\n",
    "        self.integral = 0\n",
    "        self.previous_error = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.integral = 0\n",
    "        self.previous_error = 0\n",
    "\n",
    "    def compute(self, error, dt):\n",
    "        self.integral += error * dt\n",
    "        derivative = (error - self.previous_error) / dt\n",
    "        output = self.Kp * error + self.Ki * self.integral + self.Kd * derivative\n",
    "        self.previous_error = error\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PID 게인을 설정합니다.\n",
    "\n",
    "### [TODO] PID 게인을 설정합니다. P,I,D 게인을 각각 5.0, 0.01, 0.5 로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PID parameters\n",
    "Kp = 5.0\n",
    "Ki = 0.01\n",
    "Kd = 0.5\n",
    "\n",
    "# Create the PID controller\n",
    "pid = PIDController(Kp, Ki, Kd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Create the Pendulum environments\n",
    "env_pendulum = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "obs = env_pendulum.reset(seed=random_seed)[0]\n",
    "num_steps = 200\n",
    "\n",
    "\n",
    "def pid_controller_pendulum(obs, pid, target_theta=0, dt=0.02, max_torque=2.0):\n",
    "    # observation에서 theta를 추출\n",
    "    cos_theta = obs[0]\n",
    "    sin_theta = obs[1]\n",
    "    theta = np.arctan2(sin_theta, cos_theta)\n",
    "\n",
    "    error = target_theta - theta\n",
    "    torque = pid.compute(error, dt)\n",
    "    return [np.clip(torque, -max_torque, max_torque)] \n",
    "\n",
    "\n",
    "episode_reward = 0\n",
    "for step in range(num_steps):\n",
    "    # Get the action from the simple controller\n",
    "    action = pid_controller_pendulum(obs, pid, max_torque=1.0)\n",
    "\n",
    "    # Take a step in the environment\n",
    "    obs, reward, terminated, truncated, info = env_pendulum.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "    # Render the environment\n",
    "    render_env(env_pendulum)\n",
    "\n",
    "    #print(f\"Step {step+1}: Reward = {episode_reward:.2f}\")\n",
    "\n",
    "# Close the Pendulum environment\n",
    "env_pendulum.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PID 제어기는 모델을 사용하지 않는 제어기입니다. 따라서, 비선형 모델일 경우 성능이 좋지 않습니다.\n",
    "\n",
    "PID가 좋은 성능을 발휘할 때는 모델이 선형에 가까운 경우, 즉 이 환경에서는 막대가 이미 안정점 근처에 있는 경우입니다.\n",
    "\n",
    "Gymnasium에서는 초기 상태를 설정하는 것이 불가능합니다. 위의 코드를 여러번 실행하면 PID가 제어에 성공하는 케이스를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. 모델 기반 제어기로 비선형 모델 제어\n",
    "\n",
    "---\n",
    "### 3.1 모델 기반 제어기 소개\n",
    "고전 제어기의 한계는 모델을 고려하지 않기 때문에 비선형 모델에서 좋은 제어 성능을 발휘할 수 없다는 것입니다.\n",
    "\n",
    "이번에는 모델을 사용하는 현대 제어기를 사용해보겠습니다.\n",
    "\n",
    "현대 제어기 중 실제로 가장 많이 사용되는 제어기는 MPC 기반의 제어기들입니다.\n",
    "\n",
    "MPC는 모델을 이용하여 미래에 모델이 어떻게 움직이는지 분석하여 최적화를 수행합니다. 따라서 비선형 모델을 효과적으로 제어할 수 있습니다.\n",
    "\n",
    "기울기 기반의 MPC인 iLQR, DDP가 잘 알려져있습니다. 기울기 기반의 MPC보다 더 빠르게 최적화를 수행할 수 있는 샘플링 기반 MPC를 실습으로 사용해보겠습니다.\n",
    "\n",
    "Model Predictive Path Integral Control (MPPI) 제어기는 여러개의 제어입력 궤적을 샘플링하여 동시에 궤적을 계산합니다. 이를 통해 최적의 제어입력을 계산할 수 있습니다. 이때, 여러개의 궤적을 GPU로 동시에 시뮬레이션할 수 있기 때문에, 실시간으로 복잡한 모델을 제어할 수 있는 강력한 제어기로 알려져있습니다.\n",
    "\n",
    "본 실습에서는 100개의 적은 궤적만 사용하므로 cpu로 진행해보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_mppi import MPPI\n",
    "import torch\n",
    "import math\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "TIMESTEPS = 15  # 예측할 미래 궤적의 시간 수\n",
    "N_SAMPLES = 100  # 동시에 샘플링할 궤적 수\n",
    "ACTION_LOW = -1.0\n",
    "ACTION_HIGH = 1.0\n",
    "\n",
    "d = \"cpu\"\n",
    "dtype = torch.double\n",
    "nx = 2\n",
    "\n",
    "noise_sigma = torch.tensor(10, device=d, dtype=dtype)\n",
    "# noise_sigma = torch.tensor([[10, 0], [0, 10]], device=d, dtype=dtype)\n",
    "lambda_ = 1.\n",
    "\n",
    "# pendulum의 동역학\n",
    "def dynamics(state, perturbed_action):\n",
    "        # true dynamics from gym\n",
    "        th = state[:, 0].view(-1, 1)\n",
    "        thdot = state[:, 1].view(-1, 1)\n",
    "\n",
    "        g = 10\n",
    "        m = 1\n",
    "        l = 1\n",
    "        dt = 0.05\n",
    "\n",
    "        u = perturbed_action\n",
    "        u = torch.clamp(u, -2, 2)\n",
    "\n",
    "        newthdot = thdot + (3 * g / (2 * l) * np.sin(th) + 3.0 / (m * l ** 2) * u) * dt\n",
    "        newthdot = np.clip(newthdot, -8, 8)\n",
    "        newth = th + newthdot * dt\n",
    "\n",
    "        state = torch.cat((newth, newthdot), dim=1)\n",
    "        return state\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return (((x + math.pi) % (2 * math.pi)) - math.pi)\n",
    "\n",
    "\n",
    "def running_cost(state, action):\n",
    "    theta = state[:, 0]\n",
    "    theta_dt = state[:, 1]\n",
    "    action = action[:, 0]\n",
    "    cost = angle_normalize(theta) ** 2 + 0.1 * theta_dt ** 2\n",
    "    return cost\n",
    "\n",
    "\n",
    "# create controller with chosen parameters\n",
    "ctrl = MPPI(dynamics, running_cost, nx, noise_sigma, num_samples=N_SAMPLES, horizon=TIMESTEPS,\n",
    "            lambda_=lambda_, device=d,\n",
    "            u_min=torch.tensor(ACTION_LOW, dtype=torch.double, device=d),\n",
    "            u_max=torch.tensor(ACTION_HIGH, dtype=torch.double, device=d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### 3.2 Inverted Pendulum 모델에 적용\n",
    "pendulum 모델에 적용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the Pendulum environments\n",
    "env_pendulum = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "obs = env_pendulum.reset(seed=random_seed)[0]\n",
    "num_steps = 200\n",
    "\n",
    "episode_reward = 0\n",
    "for step in range(num_steps):\n",
    "    # observation에서 theta를 추출\n",
    "    cos_theta = obs[0]\n",
    "    sin_theta = obs[1]\n",
    "    theta = np.arctan2(sin_theta, cos_theta)\n",
    "    obs = np.array([[theta, obs[2]]])\n",
    "\n",
    "    action = ctrl.command(obs)\n",
    "    action = action.cpu().numpy()\n",
    "\n",
    "    # Take a step in the environment\n",
    "    obs, reward, terminated, truncated, info = env_pendulum.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "    # Render the environment\n",
    "    render_env(env_pendulum)\n",
    "\n",
    "    #print(f\"Step {step+1}: Reward = {episode_reward:.2f}\")\n",
    "\n",
    "# Close the Pendulum environment\n",
    "env_pendulum.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최대 제어 토크를 1로 설정해도, 중력을 이용해 막대를 스윙하며 제어에 성공하는 것을 볼 수 있습니다.\n",
    "\n",
    "모델 기반 제어기와 달리, 시스템의 모델을 몰라도 제어가 가능한 강화학습 알고리즘들이 주목을 받고 있습니다.\n",
    "\n",
    "또, 데이터가 충분하다면, 모델이 불확실하고 시간에 따라 변하는 상황에서도 강건하게 제어를 할 수 있습니다.\n",
    "\n",
    "그리고 제어 입력이 이산화되어도 효과적으로 제어를 할 수 있다고 알려져있습니다. 이를 실습을 통해 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Reinforce 강화학습 알고리즘 적용\n",
    "\n",
    "---\n",
    "### 4.1 Reinforce 알고리즘 구현\n",
    "Reinforce 알고리즘은 강화학습의 가장 기본이 되는 알고리즘입니다.\n",
    "\n",
    "강화학습에서 가장 기본으로 사용되는 라이브러리인 pytorch를 이용해 Reinforce 알고리즘을 구현해보겠습니다.\n",
    "\n",
    "Reinforce 알고리즘을 이용해 inverted pendulum 환경을 학습해보겠습니다.\n",
    "\n",
    "Reinforce는 가장 기초적인 알고리즘으로, 학습이 느리고 다양한 환경에서 잘 동작하지 않는다는 단점이 있습니다.\n",
    "\n",
    "Inverted pendulum 문제는 제어 공간이 연속적입니다. 연속적인 제어 공간을 다룰 수 있는 여러가지 강화학습 알고리즘들이 있습니다 (ddpg 등).\n",
    "\n",
    "이번 실습에서는 연속된 제어 공간을 위해 알고리즘을 수정하지 않고, Reinforce 알고리즘의 특성을 간접적으로 알아보겠습니다.\n",
    "\n",
    "제어 입력인 토크를 -1, 0, 1로 설정하고 알고리즘을 설계해보겠습니다. 따라서 모델의 출력층 크기는 3이 되어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0008\n",
    "gamma         = 0.98\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(3, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=0)\n",
    "        return x\n",
    "      \n",
    "    def put_data(self, item):\n",
    "        self.data.append(item)\n",
    "        \n",
    "    def train_net(self):\n",
    "        R = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        for r, prob in self.data[::-1]:\n",
    "            R = r + gamma * R\n",
    "            loss = -torch.log(prob) * R\n",
    "            loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.2 Inverted pedulum 에 적용\n",
    "알고리즘을 이용해 학습해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : -1484.9236340299035\n",
      "# of episode :40, avg score : -1320.6297421659456\n",
      "# of episode :60, avg score : -1286.3529628437636\n",
      "# of episode :80, avg score : -1310.6425121216344\n",
      "# of episode :100, avg score : -1321.2304771910744\n",
      "# of episode :120, avg score : -1391.4460186860272\n",
      "# of episode :140, avg score : -1364.0800863521583\n",
      "# of episode :160, avg score : -1382.8123973824872\n",
      "# of episode :180, avg score : -1560.9597163399797\n",
      "# of episode :200, avg score : -1590.0002099829512\n",
      "# of episode :220, avg score : -1530.1718459460765\n",
      "# of episode :240, avg score : -1485.9770459962547\n",
      "# of episode :260, avg score : -1471.3956077106945\n",
      "# of episode :280, avg score : -1510.8351879576287\n",
      "# of episode :300, avg score : -1590.3896826237817\n",
      "# of episode :320, avg score : -1634.9031071049144\n",
      "# of episode :340, avg score : -1632.037028193759\n",
      "# of episode :360, avg score : -1645.4738903489476\n",
      "# of episode :380, avg score : -1674.8286100119844\n",
      "# of episode :400, avg score : -1661.1681399150355\n",
      "# of episode :420, avg score : -1638.1741698465423\n",
      "# of episode :440, avg score : -1565.2785210361262\n",
      "# of episode :460, avg score : -1565.7670436364397\n",
      "# of episode :480, avg score : -1476.5091401279028\n",
      "# of episode :500, avg score : -1235.4732138672334\n",
      "# of episode :520, avg score : -1097.0017636320101\n",
      "# of episode :540, avg score : -1265.6936395021858\n",
      "# of episode :560, avg score : -1160.605041421131\n",
      "# of episode :580, avg score : -1103.114045147384\n",
      "# of episode :600, avg score : -1102.8419950712305\n",
      "# of episode :620, avg score : -1208.2276314414025\n",
      "# of episode :640, avg score : -1112.369099701964\n",
      "# of episode :660, avg score : -1147.4653079805164\n",
      "# of episode :680, avg score : -1198.5745125621286\n",
      "# of episode :700, avg score : -1188.4862995550236\n",
      "# of episode :720, avg score : -1161.8211758888185\n",
      "# of episode :740, avg score : -1152.5034938348158\n",
      "# of episode :760, avg score : -1165.6768296443338\n",
      "# of episode :780, avg score : -1115.7148382187734\n",
      "# of episode :800, avg score : -1098.8086645176722\n",
      "# of episode :820, avg score : -1115.5376492070131\n",
      "# of episode :840, avg score : -1108.5895711966693\n",
      "# of episode :860, avg score : -1141.0837168213798\n",
      "# of episode :880, avg score : -1058.5402272706247\n",
      "# of episode :900, avg score : -1118.8378989783685\n",
      "# of episode :920, avg score : -1131.8021586075033\n",
      "# of episode :940, avg score : -1127.8597028998786\n",
      "# of episode :960, avg score : -1142.5663553248673\n",
      "# of episode :980, avg score : -1143.3290752422822\n",
      "# of episode :1000, avg score : -1173.3370912429973\n",
      "# of episode :1020, avg score : -1141.8559915157662\n",
      "# of episode :1040, avg score : -1204.3931826051453\n",
      "# of episode :1060, avg score : -1237.466083408455\n",
      "# of episode :1080, avg score : -1242.378546933079\n",
      "# of episode :1100, avg score : -1262.66087849114\n",
      "# of episode :1120, avg score : -1227.8916081190405\n",
      "# of episode :1140, avg score : -1245.421355469843\n",
      "# of episode :1160, avg score : -1261.6747834193934\n",
      "# of episode :1180, avg score : -1258.3542429474353\n",
      "# of episode :1200, avg score : -1245.7205742202214\n",
      "# of episode :1220, avg score : -1099.435778597102\n",
      "# of episode :1240, avg score : -1158.6083897215315\n",
      "# of episode :1260, avg score : -1285.3708324123354\n",
      "# of episode :1280, avg score : -1283.4951374848579\n",
      "# of episode :1300, avg score : -1309.4988182245997\n",
      "# of episode :1320, avg score : -1244.7535807149836\n",
      "# of episode :1340, avg score : -1318.7598755173237\n",
      "# of episode :1360, avg score : -1331.462676372152\n",
      "# of episode :1380, avg score : -1342.2317022689367\n",
      "# of episode :1400, avg score : -1345.5597466238535\n",
      "# of episode :1420, avg score : -1351.690547157738\n",
      "# of episode :1440, avg score : -1352.7804713074015\n",
      "# of episode :1460, avg score : -1349.3781919731496\n",
      "# of episode :1480, avg score : -1339.8996832115872\n",
      "# of episode :1500, avg score : -1350.0200174914098\n",
      "# of episode :1520, avg score : -1350.8761456559696\n",
      "# of episode :1540, avg score : -1349.555397480553\n",
      "# of episode :1560, avg score : -1348.7043806767285\n",
      "# of episode :1580, avg score : -1342.3520116982165\n",
      "# of episode :1600, avg score : -1345.3498720879884\n",
      "# of episode :1620, avg score : -1342.6780201165188\n",
      "# of episode :1640, avg score : -1346.577678730759\n",
      "# of episode :1660, avg score : -1347.876469119305\n",
      "# of episode :1680, avg score : -1348.6529501860555\n",
      "# of episode :1700, avg score : -1347.5942595187985\n",
      "# of episode :1720, avg score : -1349.4417323469097\n",
      "# of episode :1740, avg score : -1347.8154657884538\n",
      "# of episode :1760, avg score : -1345.4153526146363\n",
      "# of episode :1780, avg score : -1349.9486375730453\n",
      "# of episode :1800, avg score : -1350.4856407288783\n",
      "# of episode :1820, avg score : -1351.4424048047972\n",
      "# of episode :1840, avg score : -1347.8688785987595\n",
      "# of episode :1860, avg score : -1352.3909534372872\n",
      "# of episode :1880, avg score : -1353.0917608008056\n",
      "# of episode :1900, avg score : -1353.7640631493327\n",
      "# of episode :1920, avg score : -1353.8821782096281\n",
      "# of episode :1940, avg score : -1353.7024454626649\n",
      "# of episode :1960, avg score : -1352.6094683294707\n",
      "# of episode :1980, avg score : -1353.8284064713066\n",
      "# of episode :2000, avg score : -1353.500722696681\n",
      "# of episode :2020, avg score : -1354.0991476269126\n",
      "# of episode :2040, avg score : -1353.5282050442795\n",
      "# of episode :2060, avg score : -1353.344982621434\n",
      "# of episode :2080, avg score : -1353.6671421905808\n",
      "# of episode :2100, avg score : -1353.9717225096624\n",
      "# of episode :2120, avg score : -1354.7676202334137\n",
      "# of episode :2140, avg score : -1354.919954980138\n",
      "# of episode :2160, avg score : -1353.9773565351993\n",
      "# of episode :2180, avg score : -1354.637975661476\n",
      "# of episode :2200, avg score : -1354.2494497889613\n",
      "# of episode :2220, avg score : -1355.3362253982891\n",
      "# of episode :2240, avg score : -1354.46386786277\n",
      "# of episode :2260, avg score : -1353.9935608663234\n",
      "# of episode :2280, avg score : -1354.202798747318\n",
      "# of episode :2300, avg score : -1354.5686578819057\n",
      "# of episode :2320, avg score : -1354.1498348948653\n",
      "# of episode :2340, avg score : -1352.9763087805538\n",
      "# of episode :2360, avg score : -1353.4199108709083\n",
      "# of episode :2380, avg score : -1353.060318319447\n",
      "# of episode :2400, avg score : -1352.508274169647\n",
      "# of episode :2420, avg score : -1353.3733610178433\n",
      "# of episode :2440, avg score : -1354.1151561339595\n",
      "# of episode :2460, avg score : -1353.9693132339867\n",
      "# of episode :2480, avg score : -1352.2303105962978\n",
      "# of episode :2500, avg score : -1354.4076848376112\n",
      "# of episode :2520, avg score : -1354.1904938536918\n",
      "# of episode :2540, avg score : -1354.1831465096836\n",
      "# of episode :2560, avg score : -1354.89143644903\n",
      "# of episode :2580, avg score : -1354.3047176353182\n",
      "# of episode :2600, avg score : -1353.451647347544\n",
      "# of episode :2620, avg score : -1354.2180266615028\n",
      "# of episode :2640, avg score : -1354.5340837163217\n",
      "# of episode :2660, avg score : -1354.5446893598407\n",
      "# of episode :2680, avg score : -1355.1111018648046\n",
      "# of episode :2700, avg score : -1354.647057214734\n",
      "# of episode :2720, avg score : -1355.7241506503972\n",
      "# of episode :2740, avg score : -1354.8415597023397\n",
      "# of episode :2760, avg score : -1355.0920973026982\n",
      "# of episode :2780, avg score : -1354.804869138958\n",
      "# of episode :2800, avg score : -1355.1494691228022\n",
      "# of episode :2820, avg score : -1354.637928515132\n",
      "# of episode :2840, avg score : -1354.6779359367815\n",
      "# of episode :2860, avg score : -1354.5133455170012\n",
      "# of episode :2880, avg score : -1355.5987480218905\n",
      "# of episode :2900, avg score : -1354.7275688274933\n",
      "# of episode :2920, avg score : -1354.4387255218235\n",
      "# of episode :2940, avg score : -1355.321663472832\n",
      "# of episode :2960, avg score : -1355.1175488087417\n",
      "# of episode :2980, avg score : -1355.7102628571658\n",
      "# of episode :3000, avg score : -1354.5027965432037\n",
      "# of episode :3020, avg score : -1354.86702361671\n",
      "# of episode :3040, avg score : -1355.324570814424\n",
      "# of episode :3060, avg score : -1354.2676941474733\n",
      "# of episode :3080, avg score : -1355.2027588153128\n",
      "# of episode :3100, avg score : -1354.749868842176\n",
      "# of episode :3120, avg score : -1354.3143269121465\n",
      "# of episode :3140, avg score : -1354.4319402593385\n",
      "# of episode :3160, avg score : -1354.760906048208\n",
      "# of episode :3180, avg score : -1355.1263912393672\n",
      "# of episode :3200, avg score : -1354.4629438169138\n",
      "# of episode :3220, avg score : -1354.3945940746862\n",
      "# of episode :3240, avg score : -1353.7501888887957\n",
      "# of episode :3260, avg score : -1354.2945572574984\n",
      "# of episode :3280, avg score : -1354.7321137703595\n",
      "# of episode :3300, avg score : -1354.964896645129\n",
      "# of episode :3320, avg score : -1354.634181488799\n",
      "# of episode :3340, avg score : -1352.5437722669599\n",
      "# of episode :3360, avg score : -1352.2996265597883\n",
      "# of episode :3380, avg score : -1350.2059796791514\n",
      "# of episode :3400, avg score : -1346.7087643469522\n",
      "# of episode :3420, avg score : -1343.820187039297\n",
      "# of episode :3440, avg score : -1344.1884026609084\n",
      "# of episode :3460, avg score : -1280.478998346649\n",
      "# of episode :3480, avg score : -1245.406808588847\n",
      "# of episode :3500, avg score : -1258.497503330123\n",
      "# of episode :3520, avg score : -1224.060586389698\n",
      "# of episode :3540, avg score : -1298.9252783572692\n",
      "# of episode :3560, avg score : -1268.2717265959714\n",
      "# of episode :3580, avg score : -1120.8922187167616\n",
      "# of episode :3600, avg score : -1054.5489647425231\n",
      "# of episode :3620, avg score : -1055.4892252926127\n",
      "# of episode :3640, avg score : -1102.6687961495331\n",
      "# of episode :3660, avg score : -1064.2128652054366\n",
      "# of episode :3680, avg score : -1086.4557339211103\n",
      "# of episode :3700, avg score : -1076.8282824372445\n",
      "# of episode :3720, avg score : -1036.8749778955323\n",
      "# of episode :3740, avg score : -1062.2661497505064\n",
      "# of episode :3760, avg score : -1052.9120304258925\n",
      "# of episode :3780, avg score : -1061.5251808728856\n",
      "# of episode :3800, avg score : -1060.0359266468786\n",
      "# of episode :3820, avg score : -1064.8024045517407\n",
      "# of episode :3840, avg score : -1058.1254248146984\n",
      "# of episode :3860, avg score : -1061.9943824329475\n",
      "# of episode :3880, avg score : -1072.0913248791708\n",
      "# of episode :3900, avg score : -1056.4201202007125\n",
      "# of episode :3920, avg score : -1068.608746589306\n",
      "# of episode :3940, avg score : -1065.8883022518264\n",
      "# of episode :3960, avg score : -1067.6372746738098\n",
      "# of episode :3980, avg score : -1068.5472154238855\n",
      "# of episode :4000, avg score : -1071.0392504058186\n",
      "# of episode :4020, avg score : -1063.874960012017\n",
      "# of episode :4040, avg score : -1060.6402441603962\n",
      "# of episode :4060, avg score : -1065.2876978105785\n",
      "# of episode :4080, avg score : -1059.5419937468225\n",
      "# of episode :4100, avg score : -1061.0429549170474\n",
      "# of episode :4120, avg score : -1065.4793604371625\n",
      "# of episode :4140, avg score : -1058.092094741121\n",
      "# of episode :4160, avg score : -1065.4676070178289\n",
      "# of episode :4180, avg score : -1068.0156871548666\n",
      "# of episode :4200, avg score : -1056.8817059350386\n",
      "# of episode :4220, avg score : -1044.0448725318054\n",
      "# of episode :4240, avg score : -1035.39751011457\n",
      "# of episode :4260, avg score : -1044.20269289247\n",
      "# of episode :4280, avg score : -1059.4432124368807\n",
      "# of episode :4300, avg score : -1053.2469231087655\n",
      "# of episode :4320, avg score : -1037.2060459518389\n",
      "# of episode :4340, avg score : -1069.169209405238\n",
      "# of episode :4360, avg score : -1066.4052696266167\n",
      "# of episode :4380, avg score : -1032.8153315102363\n",
      "# of episode :4400, avg score : -1070.737934225006\n",
      "# of episode :4420, avg score : -1064.8240900356072\n",
      "# of episode :4440, avg score : -1064.369457629224\n",
      "# of episode :4460, avg score : -1053.3045915094558\n",
      "# of episode :4480, avg score : -1053.0848243422613\n",
      "# of episode :4500, avg score : -1089.5037996146416\n",
      "# of episode :4520, avg score : -1106.3064459617028\n",
      "# of episode :4540, avg score : -1104.2863083550492\n",
      "# of episode :4560, avg score : -1046.8165646912894\n",
      "# of episode :4580, avg score : -1049.7995175414312\n",
      "# of episode :4600, avg score : -1071.468276973773\n",
      "# of episode :4620, avg score : -1055.2243820811632\n",
      "# of episode :4640, avg score : -1093.0152707866164\n",
      "# of episode :4660, avg score : -1092.7292774590337\n",
      "# of episode :4680, avg score : -1031.0415997377672\n",
      "# of episode :4700, avg score : -1061.9210160615614\n",
      "# of episode :4720, avg score : -1116.6318213613226\n",
      "# of episode :4740, avg score : -1262.1860007148198\n",
      "# of episode :4760, avg score : -1095.8110427637716\n",
      "# of episode :4780, avg score : -1071.5964797387594\n",
      "# of episode :4800, avg score : -1106.798166971796\n",
      "# of episode :4820, avg score : -1152.8738643906406\n",
      "# of episode :4840, avg score : -1118.951163257932\n",
      "# of episode :4860, avg score : -1097.0885421198595\n",
      "# of episode :4880, avg score : -1067.3889055745253\n",
      "# of episode :4900, avg score : -1061.9438489903564\n",
      "# of episode :4920, avg score : -1055.267130868597\n",
      "# of episode :4940, avg score : -1036.6600988786295\n",
      "# of episode :4960, avg score : -1053.370023930655\n",
      "# of episode :4980, avg score : -1159.637152443171\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "pi = Policy()\n",
    "score = 0.0\n",
    "print_interval = 20\n",
    "max_steps_per_episode = 200\n",
    "\n",
    "action_map = [-1, 0, 1]  # Define the action mapping\n",
    "\n",
    "for n_epi in range(5000):\n",
    "    s, _ = env.reset(seed=random_seed)\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    while not done and step_count < max_steps_per_episode: # CartPole-v1 forced to terminates at 500 step.\n",
    "\n",
    "        step_count += 1\n",
    "        prob = pi(torch.from_numpy(s).float())\n",
    "\n",
    "        m = Categorical(prob)\n",
    "        a = m.sample()\n",
    "        action = action_map[a.item()]\n",
    "\n",
    "        s_prime, r, done, truncated, info = env.step([action])\n",
    "        pi.put_data((r,prob[a]))\n",
    "        s = s_prime\n",
    "        score += r\n",
    "        \n",
    "    pi.train_net()\n",
    "    \n",
    "    if n_epi%print_interval==0 and n_epi!=0:\n",
    "        print(\"# of episode :{}, avg score : {}\".format(n_epi, score/print_interval))\n",
    "        score = 0.0\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 완료된 모델을 불러와 다양한 초기조건에서 막대를 제어해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi.eval()\n",
    "env_pendulum = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "s, _ = env_pendulum.reset(seed=random_seed)\n",
    "\n",
    "num_steps = 200\n",
    "\n",
    "episode_reward = 0\n",
    "for step in range(num_steps):\n",
    "\n",
    "    prob = pi(torch.from_numpy(s).float())\n",
    "    a = prob.argmax().item()  # Select the action with the highest probability\n",
    "    s, r, done, truncated, info = env_pendulum.step([a])\n",
    "\n",
    "    # Render the environment\n",
    "    render_env(env_pendulum)\n",
    "\n",
    "# Close the Pendulum environment\n",
    "env_pendulum.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제어 결과, 학습이 잘 이뤄지지 않은 것을 볼 수 있습니다.\n",
    "\n",
    "학습 iteration 수를 크게 늘리면 도움이 될 수 있지만, 다른 최신 강화학습 알고리즘을 사용하는 것이 더 합리적일 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.3 Cartpole 에 적용\n",
    "Reinforce 알고리즘을 이용해 Cartpole 환경을 학습해보겠습니다.\n",
    "policy의 입력과 출력층을 cartpole에 맞게 변경합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "gamma         = 0.98\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=0)\n",
    "        return x\n",
    "      \n",
    "    def put_data(self, item):\n",
    "        self.data.append(item)\n",
    "        \n",
    "    def train_net(self):\n",
    "        R = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        for r, prob in self.data[::-1]:\n",
    "            R = r + gamma * R\n",
    "            loss = -torch.log(prob) * R\n",
    "            loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cartpole 환경에서 Reinforce 알고리즘을 학습시킵니다. \n",
    "\n",
    "학습에 시간이 조금 소요됩니다.\n",
    "\n",
    "### [TODO] 환경에 적용할 제어 입력 a를 구합니다. 제어 정책의 출력을 카테고리 데이터로 변환한 변수에서 샘플링을 진행하여 획득할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 19.85\n",
      "# of episode :40, avg score : 20.35\n",
      "# of episode :60, avg score : 19.15\n",
      "# of episode :80, avg score : 26.2\n",
      "# of episode :100, avg score : 24.6\n",
      "# of episode :120, avg score : 23.05\n",
      "# of episode :140, avg score : 25.4\n",
      "# of episode :160, avg score : 24.05\n",
      "# of episode :180, avg score : 26.2\n",
      "# of episode :200, avg score : 22.6\n",
      "# of episode :220, avg score : 25.45\n",
      "# of episode :240, avg score : 29.8\n",
      "# of episode :260, avg score : 25.2\n",
      "# of episode :280, avg score : 31.55\n",
      "# of episode :300, avg score : 29.5\n",
      "# of episode :320, avg score : 35.45\n",
      "# of episode :340, avg score : 37.7\n",
      "# of episode :360, avg score : 31.65\n",
      "# of episode :380, avg score : 33.7\n",
      "# of episode :400, avg score : 35.5\n",
      "# of episode :420, avg score : 39.85\n",
      "# of episode :440, avg score : 40.5\n",
      "# of episode :460, avg score : 48.05\n",
      "# of episode :480, avg score : 40.55\n",
      "# of episode :500, avg score : 44.75\n",
      "# of episode :520, avg score : 37.8\n",
      "# of episode :540, avg score : 35.5\n",
      "# of episode :560, avg score : 34.8\n",
      "# of episode :580, avg score : 50.85\n",
      "# of episode :600, avg score : 60.3\n",
      "# of episode :620, avg score : 44.85\n",
      "# of episode :640, avg score : 50.4\n",
      "# of episode :660, avg score : 53.75\n",
      "# of episode :680, avg score : 43.6\n",
      "# of episode :700, avg score : 55.4\n",
      "# of episode :720, avg score : 42.7\n",
      "# of episode :740, avg score : 55.3\n",
      "# of episode :760, avg score : 55.75\n",
      "# of episode :780, avg score : 61.4\n",
      "# of episode :800, avg score : 59.8\n",
      "# of episode :820, avg score : 65.4\n",
      "# of episode :840, avg score : 50.2\n",
      "# of episode :860, avg score : 57.25\n",
      "# of episode :880, avg score : 60.25\n",
      "# of episode :900, avg score : 69.65\n",
      "# of episode :920, avg score : 67.05\n",
      "# of episode :940, avg score : 70.35\n",
      "# of episode :960, avg score : 72.65\n",
      "# of episode :980, avg score : 64.7\n",
      "# of episode :1000, avg score : 92.4\n",
      "# of episode :1020, avg score : 69.65\n",
      "# of episode :1040, avg score : 75.25\n",
      "# of episode :1060, avg score : 63.55\n",
      "# of episode :1080, avg score : 110.05\n",
      "# of episode :1100, avg score : 99.5\n",
      "# of episode :1120, avg score : 86.6\n",
      "# of episode :1140, avg score : 93.7\n",
      "# of episode :1160, avg score : 114.6\n",
      "# of episode :1180, avg score : 133.75\n",
      "# of episode :1200, avg score : 105.15\n",
      "# of episode :1220, avg score : 109.7\n",
      "# of episode :1240, avg score : 159.45\n",
      "# of episode :1260, avg score : 119.3\n",
      "# of episode :1280, avg score : 157.55\n",
      "# of episode :1300, avg score : 140.35\n",
      "# of episode :1320, avg score : 155.75\n",
      "# of episode :1340, avg score : 151.1\n",
      "# of episode :1360, avg score : 154.25\n",
      "# of episode :1380, avg score : 167.0\n",
      "# of episode :1400, avg score : 177.85\n",
      "# of episode :1420, avg score : 195.7\n",
      "# of episode :1440, avg score : 176.0\n",
      "# of episode :1460, avg score : 222.8\n",
      "# of episode :1480, avg score : 173.45\n",
      "# of episode :1500, avg score : 186.7\n",
      "# of episode :1520, avg score : 195.35\n",
      "# of episode :1540, avg score : 215.85\n",
      "# of episode :1560, avg score : 216.8\n",
      "# of episode :1580, avg score : 200.5\n",
      "# of episode :1600, avg score : 211.25\n",
      "# of episode :1620, avg score : 215.7\n",
      "# of episode :1640, avg score : 296.6\n",
      "# of episode :1660, avg score : 211.1\n",
      "# of episode :1680, avg score : 266.3\n",
      "# of episode :1700, avg score : 228.2\n",
      "# of episode :1720, avg score : 208.6\n",
      "# of episode :1740, avg score : 257.85\n",
      "# of episode :1760, avg score : 285.15\n",
      "# of episode :1780, avg score : 286.2\n",
      "# of episode :1800, avg score : 287.9\n",
      "# of episode :1820, avg score : 237.9\n",
      "# of episode :1840, avg score : 323.6\n",
      "# of episode :1860, avg score : 296.85\n",
      "# of episode :1880, avg score : 285.3\n",
      "# of episode :1900, avg score : 343.3\n",
      "# of episode :1920, avg score : 312.65\n",
      "# of episode :1940, avg score : 292.65\n",
      "# of episode :1960, avg score : 319.8\n",
      "# of episode :1980, avg score : 270.45\n",
      "# of episode :2000, avg score : 277.35\n",
      "# of episode :2020, avg score : 302.0\n",
      "# of episode :2040, avg score : 322.8\n",
      "# of episode :2060, avg score : 329.25\n",
      "# of episode :2080, avg score : 321.7\n",
      "# of episode :2100, avg score : 287.35\n",
      "# of episode :2120, avg score : 295.9\n",
      "# of episode :2140, avg score : 380.05\n",
      "# of episode :2160, avg score : 365.4\n",
      "# of episode :2180, avg score : 334.15\n",
      "# of episode :2200, avg score : 332.25\n",
      "# of episode :2220, avg score : 361.3\n",
      "# of episode :2240, avg score : 406.05\n",
      "# of episode :2260, avg score : 452.2\n",
      "# of episode :2280, avg score : 394.65\n",
      "# of episode :2300, avg score : 356.2\n",
      "# of episode :2320, avg score : 290.3\n",
      "# of episode :2340, avg score : 358.85\n",
      "# of episode :2360, avg score : 463.85\n",
      "# of episode :2380, avg score : 330.2\n",
      "# of episode :2400, avg score : 358.4\n",
      "# of episode :2420, avg score : 369.35\n",
      "# of episode :2440, avg score : 381.1\n",
      "# of episode :2460, avg score : 308.6\n",
      "# of episode :2480, avg score : 286.55\n",
      "# of episode :2500, avg score : 326.7\n",
      "# of episode :2520, avg score : 316.85\n",
      "# of episode :2540, avg score : 311.9\n",
      "# of episode :2560, avg score : 300.15\n",
      "# of episode :2580, avg score : 311.15\n",
      "# of episode :2600, avg score : 346.05\n",
      "# of episode :2620, avg score : 352.0\n",
      "# of episode :2640, avg score : 346.85\n",
      "# of episode :2660, avg score : 362.65\n",
      "# of episode :2680, avg score : 394.65\n",
      "# of episode :2700, avg score : 445.9\n",
      "# of episode :2720, avg score : 341.7\n",
      "# of episode :2740, avg score : 357.8\n",
      "# of episode :2760, avg score : 510.25\n",
      "# of episode :2780, avg score : 418.75\n",
      "# of episode :2800, avg score : 413.3\n",
      "# of episode :2820, avg score : 550.75\n",
      "# of episode :2840, avg score : 556.5\n",
      "# of episode :2860, avg score : 481.55\n",
      "# of episode :2880, avg score : 404.15\n",
      "# of episode :2900, avg score : 422.05\n",
      "# of episode :2920, avg score : 377.3\n",
      "# of episode :2940, avg score : 293.0\n",
      "# of episode :2960, avg score : 314.35\n",
      "# of episode :2980, avg score : 284.8\n",
      "# of episode :3000, avg score : 277.35\n",
      "# of episode :3020, avg score : 273.85\n",
      "# of episode :3040, avg score : 305.95\n",
      "# of episode :3060, avg score : 450.5\n",
      "# of episode :3080, avg score : 554.7\n",
      "# of episode :3100, avg score : 409.3\n",
      "# of episode :3120, avg score : 535.55\n",
      "# of episode :3140, avg score : 562.05\n",
      "# of episode :3160, avg score : 614.8\n",
      "# of episode :3180, avg score : 634.3\n",
      "# of episode :3200, avg score : 501.15\n",
      "# of episode :3220, avg score : 508.6\n",
      "# of episode :3240, avg score : 500.15\n",
      "# of episode :3260, avg score : 639.0\n",
      "# of episode :3280, avg score : 624.0\n",
      "# of episode :3300, avg score : 698.7\n",
      "# of episode :3320, avg score : 654.15\n",
      "# of episode :3340, avg score : 817.8\n",
      "# of episode :3360, avg score : 565.9\n",
      "# of episode :3380, avg score : 621.3\n",
      "# of episode :3400, avg score : 609.1\n",
      "# of episode :3420, avg score : 570.1\n",
      "# of episode :3440, avg score : 566.6\n",
      "# of episode :3460, avg score : 626.0\n",
      "# of episode :3480, avg score : 645.8\n",
      "# of episode :3500, avg score : 534.7\n",
      "# of episode :3520, avg score : 575.1\n",
      "# of episode :3540, avg score : 796.85\n",
      "# of episode :3560, avg score : 628.5\n",
      "# of episode :3580, avg score : 596.4\n",
      "# of episode :3600, avg score : 557.55\n",
      "# of episode :3620, avg score : 819.45\n",
      "# of episode :3640, avg score : 475.05\n",
      "# of episode :3660, avg score : 569.1\n",
      "# of episode :3680, avg score : 613.65\n",
      "# of episode :3700, avg score : 422.95\n",
      "# of episode :3720, avg score : 351.6\n",
      "# of episode :3740, avg score : 374.65\n",
      "# of episode :3760, avg score : 366.15\n",
      "# of episode :3780, avg score : 366.3\n",
      "# of episode :3800, avg score : 455.05\n",
      "# of episode :3820, avg score : 422.15\n",
      "# of episode :3840, avg score : 459.0\n",
      "# of episode :3860, avg score : 558.1\n",
      "# of episode :3880, avg score : 509.4\n",
      "# of episode :3900, avg score : 513.75\n",
      "# of episode :3920, avg score : 476.75\n",
      "# of episode :3940, avg score : 677.2\n",
      "# of episode :3960, avg score : 710.1\n",
      "# of episode :3980, avg score : 580.45\n",
      "# of episode :4000, avg score : 496.8\n",
      "# of episode :4020, avg score : 550.95\n",
      "# of episode :4040, avg score : 702.2\n",
      "# of episode :4060, avg score : 621.9\n",
      "# of episode :4080, avg score : 866.65\n",
      "# of episode :4100, avg score : 740.0\n",
      "# of episode :4120, avg score : 660.85\n",
      "# of episode :4140, avg score : 484.1\n",
      "# of episode :4160, avg score : 562.9\n",
      "# of episode :4180, avg score : 547.25\n",
      "# of episode :4200, avg score : 647.65\n",
      "# of episode :4220, avg score : 678.0\n",
      "# of episode :4240, avg score : 588.2\n",
      "# of episode :4260, avg score : 762.15\n",
      "# of episode :4280, avg score : 614.75\n",
      "# of episode :4300, avg score : 757.95\n",
      "# of episode :4320, avg score : 988.0\n",
      "# of episode :4340, avg score : 786.1\n",
      "# of episode :4360, avg score : 976.45\n",
      "# of episode :4380, avg score : 924.8\n",
      "# of episode :4400, avg score : 1337.75\n",
      "# of episode :4420, avg score : 903.65\n",
      "# of episode :4440, avg score : 927.05\n",
      "# of episode :4460, avg score : 1163.9\n",
      "# of episode :4480, avg score : 841.6\n",
      "# of episode :4500, avg score : 1658.6\n",
      "# of episode :4520, avg score : 1546.75\n",
      "# of episode :4540, avg score : 1681.0\n",
      "# of episode :4560, avg score : 925.6\n",
      "# of episode :4580, avg score : 879.3\n",
      "# of episode :4600, avg score : 1345.45\n",
      "# of episode :4620, avg score : 1095.25\n",
      "# of episode :4640, avg score : 839.35\n",
      "# of episode :4660, avg score : 1176.4\n",
      "# of episode :4680, avg score : 1022.65\n",
      "# of episode :4700, avg score : 1261.95\n",
      "# of episode :4720, avg score : 928.95\n",
      "# of episode :4740, avg score : 797.85\n",
      "# of episode :4760, avg score : 786.05\n",
      "# of episode :4780, avg score : 1530.0\n",
      "# of episode :4800, avg score : 2183.0\n",
      "# of episode :4820, avg score : 1512.2\n",
      "# of episode :4840, avg score : 1326.65\n",
      "# of episode :4860, avg score : 761.15\n",
      "# of episode :4880, avg score : 675.0\n",
      "# of episode :4900, avg score : 638.7\n",
      "# of episode :4920, avg score : 602.75\n",
      "# of episode :4940, avg score : 551.45\n",
      "# of episode :4960, avg score : 507.75\n",
      "# of episode :4980, avg score : 548.55\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "pi = Policy()\n",
    "score = 0.0\n",
    "print_interval = 20\n",
    "\n",
    "\n",
    "for n_epi in range(5000):\n",
    "    s, _ = env.reset(seed=random_seed)\n",
    "    done = False\n",
    "    \n",
    "    while not done: # CartPole-v1 forced to terminates at 500 step.\n",
    "        prob = pi(torch.from_numpy(s).float())\n",
    "        m = Categorical(prob)\n",
    "        a = m.sample()\n",
    "        s_prime, r, done, truncated, info = env.step(a.item())\n",
    "        pi.put_data((r,prob[a]))\n",
    "        s = s_prime\n",
    "        score += r\n",
    "        \n",
    "    pi.train_net()\n",
    "    \n",
    "    if n_epi%print_interval==0 and n_epi!=0:\n",
    "        print(\"# of episode :{}, avg score : {}\".format(n_epi, score/print_interval))\n",
    "        score = 0.0\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 완료된 모델을 이용해 테스트해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi.eval()\n",
    "env_cartpole = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "s, _ = env_cartpole.reset(seed=random_seed)\n",
    "\n",
    "num_steps = 200\n",
    "\n",
    "episode_reward = 0\n",
    "for step in range(num_steps):\n",
    "\n",
    "    prob = pi(torch.from_numpy(s).float())\n",
    "    a = prob.argmax().item()  # Select the action with the highest probability\n",
    "    s, r, done, truncated, info = env_cartpole.step(a)\n",
    "\n",
    "    # Render the environment\n",
    "    render_env(env_cartpole)\n",
    "\n",
    "env_cartpole.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cartpole 모델은 제어 입력 공간이 이산화되어 있습니다 (왼쪽 혹은 오른쪽). 따라서 기본적인 Reinforce 알고리즘도 잘 작동하는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Actor-Critic 강화학습 알고리즘 적용\n",
    "\n",
    "---\n",
    "### 5.1 Actor-Critic  알고리즘 구현\n",
    "\n",
    "강화학습에서 가장 기본으로 사용되는 라이브러리인 pytorch를 이용해 Actor-Critic  알고리즘을 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "gamma         = 0.98\n",
    "n_rollout     = 10\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(4,256)\n",
    "        self.fc_pi = nn.Linear(256,2)\n",
    "        self.fc_v = nn.Linear(256,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "    \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s,a,r,s_prime,done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r/100.0])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            done_lst.append([done_mask])\n",
    "        \n",
    "        s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                                               torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                                               torch.tensor(done_lst, dtype=torch.float)\n",
    "        self.data = []\n",
    "        return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n",
    "  \n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done = self.make_batch()\n",
    "        td_target = r + gamma * self.v(s_prime) * done\n",
    "        delta = td_target - self.v(s)\n",
    "        \n",
    "        pi = self.pi(s, softmax_dim=1)\n",
    "        pi_a = pi.gather(1,a)\n",
    "        loss = -torch.log(pi_a) * delta.detach() + F.smooth_l1_loss(self.v(s), td_target.detach())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.2 Cartpole에 적용\n",
    "\n",
    "강화학습에서 가장 기본으로 사용되는 라이브러리인 pytorch를 이용해 Actor-Critic  알고리즘을 구현해보겠습니다.\n",
    "\n",
    "Actor-Critic 알고리즘을 이용해 Cartpole 환경을 학습시켜보겠습니다.\n",
    "\n",
    "학습에 시간이 조금 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/p1yx5lj17yx384d74tkfr6480000gn/T/ipykernel_82259/1215267816.py:41: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 19.6\n",
      "# of episode :40, avg score : 23.2\n",
      "# of episode :60, avg score : 22.7\n",
      "# of episode :80, avg score : 21.2\n",
      "# of episode :100, avg score : 26.0\n",
      "# of episode :120, avg score : 32.2\n",
      "# of episode :140, avg score : 31.1\n",
      "# of episode :160, avg score : 44.0\n",
      "# of episode :180, avg score : 49.8\n",
      "# of episode :200, avg score : 56.6\n",
      "# of episode :220, avg score : 51.0\n",
      "# of episode :240, avg score : 56.5\n",
      "# of episode :260, avg score : 50.1\n",
      "# of episode :280, avg score : 71.2\n",
      "# of episode :300, avg score : 77.2\n",
      "# of episode :320, avg score : 80.6\n",
      "# of episode :340, avg score : 97.0\n",
      "# of episode :360, avg score : 126.3\n",
      "# of episode :380, avg score : 107.5\n",
      "# of episode :400, avg score : 153.6\n",
      "# of episode :420, avg score : 245.7\n",
      "# of episode :440, avg score : 190.8\n",
      "# of episode :460, avg score : 240.3\n",
      "# of episode :480, avg score : 236.5\n",
      "# of episode :500, avg score : 221.7\n",
      "# of episode :520, avg score : 292.1\n",
      "# of episode :540, avg score : 329.2\n",
      "# of episode :560, avg score : 167.5\n",
      "# of episode :580, avg score : 125.7\n",
      "# of episode :600, avg score : 152.8\n",
      "# of episode :620, avg score : 183.4\n",
      "# of episode :640, avg score : 181.2\n",
      "# of episode :660, avg score : 367.4\n",
      "# of episode :680, avg score : 395.1\n",
      "# of episode :700, avg score : 421.4\n",
      "# of episode :720, avg score : 456.2\n",
      "# of episode :740, avg score : 286.1\n",
      "# of episode :760, avg score : 546.0\n",
      "# of episode :780, avg score : 382.7\n",
      "# of episode :800, avg score : 265.0\n",
      "# of episode :820, avg score : 257.9\n",
      "# of episode :840, avg score : 508.8\n",
      "# of episode :860, avg score : 372.4\n",
      "# of episode :880, avg score : 600.0\n",
      "# of episode :900, avg score : 968.1\n",
      "# of episode :920, avg score : 1125.2\n",
      "# of episode :940, avg score : 994.5\n",
      "# of episode :960, avg score : 1647.3\n",
      "# of episode :980, avg score : 1901.6\n",
      "# of episode :1000, avg score : 699.6\n",
      "# of episode :1020, avg score : 168.6\n",
      "# of episode :1040, avg score : 184.0\n",
      "# of episode :1060, avg score : 242.8\n",
      "# of episode :1080, avg score : 335.6\n",
      "# of episode :1100, avg score : 238.7\n",
      "# of episode :1120, avg score : 209.8\n",
      "# of episode :1140, avg score : 241.3\n",
      "# of episode :1160, avg score : 228.8\n",
      "# of episode :1180, avg score : 217.7\n",
      "# of episode :1200, avg score : 275.6\n",
      "# of episode :1220, avg score : 740.4\n",
      "# of episode :1240, avg score : 960.5\n",
      "# of episode :1260, avg score : 755.0\n",
      "# of episode :1280, avg score : 662.6\n",
      "# of episode :1300, avg score : 707.2\n",
      "# of episode :1320, avg score : 797.1\n",
      "# of episode :1340, avg score : 3201.1\n",
      "# of episode :1360, avg score : 1682.2\n",
      "# of episode :1380, avg score : 958.5\n",
      "# of episode :1400, avg score : 275.4\n",
      "# of episode :1420, avg score : 238.1\n",
      "# of episode :1440, avg score : 204.3\n",
      "# of episode :1460, avg score : 175.8\n",
      "# of episode :1480, avg score : 158.6\n",
      "# of episode :1500, avg score : 155.3\n",
      "# of episode :1520, avg score : 147.2\n",
      "# of episode :1540, avg score : 136.9\n",
      "# of episode :1560, avg score : 123.9\n",
      "# of episode :1580, avg score : 133.7\n",
      "# of episode :1600, avg score : 150.8\n",
      "# of episode :1620, avg score : 152.3\n",
      "# of episode :1640, avg score : 143.1\n",
      "# of episode :1660, avg score : 136.2\n",
      "# of episode :1680, avg score : 127.8\n",
      "# of episode :1700, avg score : 115.9\n",
      "# of episode :1720, avg score : 128.2\n",
      "# of episode :1740, avg score : 135.2\n",
      "# of episode :1760, avg score : 151.6\n",
      "# of episode :1780, avg score : 155.6\n",
      "# of episode :1800, avg score : 163.8\n",
      "# of episode :1820, avg score : 177.3\n",
      "# of episode :1840, avg score : 197.4\n",
      "# of episode :1860, avg score : 275.0\n",
      "# of episode :1880, avg score : 458.7\n",
      "# of episode :1900, avg score : 1495.3\n",
      "# of episode :1920, avg score : 589.5\n",
      "# of episode :1940, avg score : 107.5\n",
      "# of episode :1960, avg score : 150.6\n",
      "# of episode :1980, avg score : 185.3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "model = ActorCritic()    \n",
    "print_interval = 20\n",
    "score = 0.0\n",
    "\n",
    "for n_epi in range(2000):\n",
    "    done = False\n",
    "    s, _ = env.reset(seed=random_seed)\n",
    "    while not done:\n",
    "        for t in range(n_rollout):\n",
    "            prob = model.pi(torch.from_numpy(s).float())\n",
    "            m = Categorical(prob)\n",
    "            a = m.sample().item()\n",
    "            s_prime, r, done, truncated, info = env.step(a)\n",
    "            model.put_data((s,a,r,s_prime,done))\n",
    "            \n",
    "            s = s_prime\n",
    "            score += r\n",
    "            \n",
    "            if done:\n",
    "                break                     \n",
    "        \n",
    "        model.train_net()\n",
    "        \n",
    "    if n_epi%print_interval==0 and n_epi!=0:\n",
    "        print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "        score = 0.0\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 모델을 이용해 Cartpole을 제어해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pi.eval()\n",
    "env_cartpole = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "s, _ = env_cartpole.reset(seed=random_seed)\n",
    "\n",
    "num_steps = 200\n",
    "\n",
    "episode_reward = 0\n",
    "for step in range(num_steps):\n",
    "\n",
    "    prob = pi(torch.from_numpy(s).float())\n",
    "    a = prob.argmax().item()  # Select the action with the highest probability\n",
    "    s, r, done, truncated, info = env_cartpole.step(a)\n",
    "\n",
    "    # Render the environment\n",
    "    render_env(env_cartpole)\n",
    "\n",
    "env_cartpole.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. A2C 강화학습 알고리즘 적용\n",
    "\n",
    "---\n",
    "### 6.1 A2C  알고리즘 구현\n",
    "\n",
    "Actor-critic에서 향상된 알고리즘인 A2C 알고리즘을 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Hyper Parameters\n",
    "STATE_DIM = 4\n",
    "ACTION_DIM = 2\n",
    "STEP = 2000\n",
    "SAMPLE_NUMS = 30\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,action_size):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size,action_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.log_softmax(self.fc3(out))\n",
    "        return out\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "def roll_out(actor_network,task,sample_nums,value_network,init_state):\n",
    "    #task.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    is_done = False\n",
    "    final_r = 0\n",
    "    state = init_state\n",
    "\n",
    "    for j in range(sample_nums):\n",
    "        states.append(state)\n",
    "        log_softmax_action = actor_network(Variable(torch.Tensor([state])))\n",
    "        softmax_action = torch.exp(log_softmax_action)\n",
    "        action = np.random.choice(ACTION_DIM,p=softmax_action.cpu().data.numpy()[0])\n",
    "        one_hot_action = [int(k == action) for k in range(ACTION_DIM)]\n",
    "        next_state,reward,done,_,_ = task.step(action)\n",
    "        #fix_reward = -10 if done else 1\n",
    "        actions.append(one_hot_action)\n",
    "        rewards.append(reward)\n",
    "        final_state = next_state\n",
    "        state = next_state\n",
    "        if done:\n",
    "            is_done = True\n",
    "            state, _ = task.reset(seed=random_seed)\n",
    "            break\n",
    "    if not is_done:\n",
    "        final_r = value_network(Variable(torch.Tensor([final_state]))).cpu().data.numpy()\n",
    "\n",
    "    return states,actions,rewards,final_r,state\n",
    "\n",
    "def discount_reward(r, gamma,final_r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = final_r\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.2 Cartpole에 적용\n",
    "\n",
    "A2C 알고리즘을 이용해 Cartpole 환경을 학습시켜보겠습니다.\n",
    "\n",
    "알고리즘의 병렬 처리 설계 덕분에 학습 시간이 매우 빠릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/p1yx5lj17yx384d74tkfr6480000gn/T/ipykernel_82259/927451816.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.log_softmax(self.fc3(out))\n",
      "/var/folders/0p/p1yx5lj17yx384d74tkfr6480000gn/T/ipykernel_82259/927451816.py:73: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  discounted_r[t] = running_add\n",
      "/var/folders/0p/p1yx5lj17yx384d74tkfr6480000gn/T/ipykernel_82259/1429407140.py:33: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(actor_network.parameters(),0.5)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/var/folders/0p/p1yx5lj17yx384d74tkfr6480000gn/T/ipykernel_82259/1429407140.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(value_network.parameters(),0.5)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([26])) that is different to the input size (torch.Size([26, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([23])) that is different to the input size (torch.Size([23, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([22])) that is different to the input size (torch.Size([22, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/gymnasium/envs/registration.py:519: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([19])) that is different to the input size (torch.Size([19, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([25])) that is different to the input size (torch.Size([25, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 50 test result: 73.0\n",
      "step: 100 test result: 28.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([21])) that is different to the input size (torch.Size([21, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 150 test result: 104.0\n",
      "step: 200 test result: 113.0\n",
      "step: 250 test result: 89.0\n",
      "step: 300 test result: 34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/RF/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 350 test result: 39.0\n",
      "step: 400 test result: 75.0\n",
      "step: 450 test result: 55.0\n",
      "step: 500 test result: 61.0\n",
      "step: 550 test result: 150.0\n",
      "step: 600 test result: 104.0\n",
      "step: 650 test result: 144.0\n",
      "step: 700 test result: 120.0\n",
      "step: 750 test result: 9.0\n",
      "step: 800 test result: 194.0\n",
      "step: 850 test result: 200.0\n",
      "step: 900 test result: 200.0\n",
      "step: 950 test result: 200.0\n",
      "step: 1000 test result: 84.0\n",
      "step: 1050 test result: 46.0\n",
      "step: 1100 test result: 8.0\n",
      "step: 1150 test result: 22.0\n",
      "step: 1200 test result: 149.0\n",
      "step: 1250 test result: 16.0\n",
      "step: 1300 test result: 120.0\n",
      "step: 1350 test result: 200.0\n",
      "step: 1400 test result: 200.0\n",
      "step: 1450 test result: 200.0\n",
      "step: 1500 test result: 179.0\n",
      "step: 1550 test result: 200.0\n",
      "step: 1600 test result: 103.0\n",
      "step: 1650 test result: 96.0\n",
      "step: 1700 test result: 35.0\n",
      "step: 1750 test result: 41.0\n",
      "step: 1800 test result: 41.0\n",
      "step: 1850 test result: 67.0\n",
      "step: 1900 test result: 101.0\n",
      "step: 1950 test result: 65.0\n",
      "step: 2000 test result: 95.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# init a task generator for data fetching\n",
    "task = gym.make(\"CartPole-v1\")\n",
    "init_state, _ = task.reset(seed=random_seed)\n",
    "\n",
    "# init value network\n",
    "value_network = ValueNetwork(input_size = STATE_DIM,hidden_size = 40,output_size = 1)\n",
    "value_network_optim = torch.optim.Adam(value_network.parameters(),lr=0.01)\n",
    "\n",
    "# init actor network\n",
    "actor_network = ActorNetwork(STATE_DIM,40,ACTION_DIM)\n",
    "actor_network_optim = torch.optim.Adam(actor_network.parameters(),lr = 0.01)\n",
    "\n",
    "steps =[]\n",
    "task_episodes =[]\n",
    "test_results =[]\n",
    "\n",
    "for step in range(STEP):\n",
    "    states,actions,rewards,final_r,current_state = roll_out(actor_network,task,SAMPLE_NUMS,value_network,init_state)\n",
    "    init_state = current_state\n",
    "    actions_var = Variable(torch.Tensor(actions).view(-1,ACTION_DIM))\n",
    "    states_var = Variable(torch.Tensor(states).view(-1,STATE_DIM))\n",
    "\n",
    "    # train actor network\n",
    "    actor_network_optim.zero_grad()\n",
    "    log_softmax_actions = actor_network(states_var)\n",
    "    vs = value_network(states_var).detach()\n",
    "    # calculate qs\n",
    "    qs = Variable(torch.Tensor(discount_reward(rewards,0.99,final_r)))\n",
    "\n",
    "    advantages = qs - vs\n",
    "    actor_network_loss = - torch.mean(torch.sum(log_softmax_actions*actions_var,1)* advantages)\n",
    "    actor_network_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(actor_network.parameters(),0.5)\n",
    "    actor_network_optim.step()\n",
    "\n",
    "    # train value network\n",
    "    value_network_optim.zero_grad()\n",
    "    target_values = qs\n",
    "    values = value_network(states_var)\n",
    "    criterion = nn.MSELoss()\n",
    "    value_network_loss = criterion(values,target_values)\n",
    "    value_network_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(value_network.parameters(),0.5)\n",
    "    value_network_optim.step()\n",
    "\n",
    "    # Testing\n",
    "    if (step + 1) % 50== 0:\n",
    "            result = 0\n",
    "            test_task = gym.make(\"CartPole-v0\")\n",
    "            for test_epi in range(10):\n",
    "                state, _ = test_task.reset(seed=random_seed)\n",
    "                for test_step in range(200):\n",
    "                    softmax_action = torch.exp(actor_network(Variable(torch.Tensor([state]))))\n",
    "                    #print(softmax_action.data)\n",
    "                    action = np.argmax(softmax_action.data.numpy()[0])\n",
    "                    next_state,reward,done,_,_ = test_task.step(action)\n",
    "                    result += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "            print(\"step:\",step+1,\"test result:\",result/10.0)\n",
    "            steps.append(step+1)\n",
    "            test_results.append(result/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 모델을 이용해 Cartpole을 제어해보겠습니다.\n",
    "\n",
    "### [TODO] Cartpole 환경을 시각화하는 코드를 완성해주세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEWCAYAAACqitpwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAELUlEQVR4nO3ZwQ0CMQwAwTtEMeksNV21RnSAxEoImHlHjn/78DkzcwBA6FYOA4AncQEgJy4A5MQFgJy4AJATFwBy4gJATlwAyIkLADlxASAnLgDkxAWAnLgAkBMXAHLiAkBOXADIiQsAOXEBICcuAOTEBYCcuACQExcAcuICQE5cAMiJCwA5cQEgJy4A5MQFgJy4AJATFwBy4gJATlwAyIkLADlxASAnLgDkxAWAnLgAkBMXAHLiAkBOXADIiQsAOXEBICcuAOTEBYCcuACQExcAcuICQE5cAMiJCwA5cQEgJy4A5MQFgJy4AJATFwBy4gJATlwAyIkLADlxASAnLgDkxAWAnLgAkBMXAHLiAkBOXADIiQsAOXEBICcuAOTEBYCcuACQExcAcuICQE5cAMiJCwA5cQEgJy4A5MQFgJy4AJATFwBy4gJATlwAyIkLADlxASAnLgDkxAWAnLgAkBMXAHLiAkBOXADIiQsAOXEBICcuAOTEBYCcuACQExcAcuICQE5cAMiJCwA5cQEgJy4A5MQFgJy4AJATFwBy4gJATlwAyIkLADlxASAnLgDkxAWAnLgAkBMXAHLiAkBOXADIiQsAOXEBICcuAOTEBYCcuACQExcAcuICQE5cAMiJCwA5cQEgJy4A5MQFgJy4AJATFwBy4gJATlwAyIkLADlxASAnLgDkxAWAnLgAkBMXAHLiAkBOXADIiQsAOXEBICcuAOTEBYCcuACQExcAcuICQE5cAMiJCwA5cQEgJy4A5MQFgJy4AJATFwBy4gJATlwAyIkLADlxASAnLgDkxAWAnLgAkBMXAHLiAkBOXADIiQsAOXEBICcuAOTEBYCcuACQExcAcuICQE5cAMiJCwA5cQEgJy4A5MQFgJy4AJATFwBy4gJATlwAyIkLADlxASAnLgDkxAWAnLgAkBMXAHLiAkBOXADIiQsAOXEBICcuAOTEBYCcuACQExcAcuICQE5cAMiJCwA5cQEgJy4A5MQFgJy4AJATFwBy4gJATlwAyIkLALn7qw+v6+p/B+Dr7L27uKy13t0HgD9xzsx8egkAfoubCwA5cQEgJy4A5MQFgJy4AJATFwBy4gJATlwAyIkLADlxASAnLgDkxAWAnLgAkBMXAHLiAkBOXADIiQsAOXEBICcuAOTEBYCcuACQExcAcuICQE5cAMiJCwA5cQEgJy4A5MQFgJy4AJATFwBy4gJATlwAyIkLADlxASAnLgDkxAWAnLgAkBMXAHLiAkBOXADIiQsAOXEBICcuAOTEBYCcuACQExcAcuICQE5cAMiJCwA5cQEgJy4A5MQFgJy4AJATFwBy4gJATlwAyIkLADlxASAnLgDkxAWAnLgAkBMXAHLiAkBOXADIiQsAOXEBICcuAOTEBYCcuACQExcAcuICQE5cAMiJCwBH7QGIww2fFbYdUQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "env_cartpole = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "s, _ = env_cartpole.reset(seed=random_seed)\n",
    "\n",
    "num_steps = 200\n",
    "\n",
    "episode_reward = 0\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    #s = torch.from_numpy(s).float()\n",
    "    a = torch.exp(actor_network(Variable(torch.Tensor([s]))))\n",
    "    a = np.argmax(a.data.numpy()[0]) \n",
    "    s, r, done, truncated, info = env_cartpole.step(a.item())\n",
    "\n",
    "    # Render the environment\n",
    "    render_env(env_cartpole)\n",
    "env_cartpole.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
